{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Dependency locality and information locality in natural language\"\n",
    "authors:\n",
    "    - name: \"Philippos Triantafyllou\"\n",
    "    - name: \"Mikolaj Golecki\"\n",
    "    - name: \"Benoît Crabbé (for the dependency trees)\"\n",
    "date: last-modified\n",
    "toc: true\n",
    "embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wet set a global random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple tokenizer class. Roughly resembles HuggingFace tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "class NaiveTokenizer:\n",
    "  \n",
    "    def __init__(self, base_vocabulary, unk='<unk>', pad='<pad>'):\n",
    "        assert(type(base_vocabulary) == list)\n",
    "        self.unk = unk\n",
    "        self.pad = pad\n",
    "        self.vocabulary = []\n",
    "        self.types2idx  = {}\n",
    "        self.add_tokens([self.unk, self.pad] + base_vocabulary)\n",
    "\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        if not type(tokens) == list:\n",
    "            tokens = [tokens]\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in self.vocabulary:\n",
    "                self.vocabulary.append(token)\n",
    "\n",
    "        self.types2idx = {el: idx for idx, el in enumerate(self.vocabulary)}\n",
    "\n",
    "\n",
    "    def tokenize(self, string: str):\n",
    "        tokens = string.split()\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens: List[str]):\n",
    "        unkid = self.types2idx[self.unk]\n",
    "        return [self.types2idx.get(token, unkid) for token in tokens]\n",
    "\n",
    "\n",
    "    def encode(self, string):\n",
    "        tokens = self.tokenize(string)\n",
    "        return self.convert_tokens_to_ids(tokens)\n",
    "\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = [self.vocabulary[idx] for idx in ids]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "\n",
    "    def __call__(self, string):\n",
    "        return self.encode(string)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def pad_id(self):\n",
    "        return self.types2idx[self.pad]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocabulary)\n",
    "\n",
    "\n",
    "    def decode_ngram(self, ngram_sequence):\n",
    "        return self.decode(ngram[-1] for ngram in ngram_sequence)\n",
    "\n",
    "\n",
    "    def pad_batch(self, batch_codes: List[List[int]]):\n",
    "        max_len = max([len(sentence) for sentence in batch_codes])\n",
    "        padded_codes = [\n",
    "            sentence + [self.pad_id] * (max_len - len(sentence))\n",
    "            for sentence in batch_codes\n",
    "        ]\n",
    "        return torch.LongTensor(padded_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_vocabulary = \"Language models are cool .\"\n",
    "tokenizer = NaiveTokenizer(base_vocabulary.split())\n",
    "codes = tokenizer(\"Language models are not so cool .\")\n",
    "decoded = tokenizer.decode(codes)\n",
    "tokens = tokenizer.tokenize(\"Language models are not so cool .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base vocab: ['Language models are cool .']\n",
      "Tokens: ['Language', 'models', 'are', 'not', 'so', 'cool', '.']\n",
      "Codes: [2, 3, 4, 0, 0, 5, 6]\n",
      "Decoded: Language models are <unk> <unk> cool .\n",
      "Tokens: ['Language', 'models', 'are', 'not', 'so', 'cool', '.']\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "\n",
    "print(\"Base vocab:\", [base_vocabulary])\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Codes:\", codes)\n",
    "print(\"Decoded:\", decoded)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give two sentences and batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Language models are cool .\", \"Language models are not so cool .\"]\n",
    "batch = [tokenizer(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5, 6], [2, 3, 4, 0, 0, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first sentence is shorter than the second one. We need to pad. Padding also converts the batch to a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4, 5, 6, 1, 1],\n",
      "        [2, 3, 4, 0, 0, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "\n",
    "print(tokenizer.pad_batch(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training/test sets of the UD corpora:\n",
    "\n",
    "- Vocabulary to be passed to the tokenizer is a list of words;\n",
    "- Data to be passed to the dataloader is a list of strings (examples);\n",
    "- Dedicated function handles this later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the `ngramify` method that we also pad with the length of the ngram size - 1 in order to work around the common problem of ngram models: they use the first tokens as context, and don't predict them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NgramsLanguageModelDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, N: int, data: List[str], tokenizer):\n",
    "        self.N = N\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = [self.ngramify(tokenizer(sentence)) for sentence in data]\n",
    "        self.data = [ngram for sent in self.data for ngram in sent if len(ngram) == self.N]\n",
    "\n",
    "    def ngramify(self, token_list: List[str]):\n",
    "        padded_tokens = [self.tokenizer.pad_id] * (self.N - 1) + token_list\n",
    "        return [padded_tokens[i:i+self.N] for i in range(len(token_list))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence: str):\n",
    "    return sentence.replace(\".\", \" . \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mock dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "zebra_dataset = \"\"\"\n",
    "There are five houses.\n",
    "The Englishman lives in the red house.\n",
    "The Spaniard owns the dog.\n",
    "Coffee is drunk in the green house.\n",
    "The Ukrainian drinks tea.\n",
    "The green house is immediately to the right of the ivory house.\n",
    "The Old Gold smoker owns snails.\n",
    "Kools are smoked in the yellow house.\n",
    "Milk is drunk in the middle house.\n",
    "The Norwegian lives in the first house.\n",
    "The man who smokes Chesterfields lives in the house next to the man with the fox.\n",
    "Kools are smoked in the house next to the house where the horse is kept.\n",
    "The Lucky Strike smoker drinks orange juice.\n",
    "The Japanese smokes Parliaments.\n",
    "The Norwegian lives next to the blue house.\n",
    "I have lost my keys, yet now I found them.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['There', 'are', 'five', 'houses', '.', 'The', 'Englishman', 'lives', 'in', 'the']\n",
      "Dataset: ['Language models are cool .', 'Language models are not so cool .']\n"
     ]
    }
   ],
   "source": [
    "base_vocabulary = normalize(zebra_dataset).split()\n",
    "data = [normalize(sent) for sent in zebra_dataset.split('\\n')]\n",
    "print(\"Vocabulary:\", base_vocabulary[:10])\n",
    "print(\"Dataset:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a tokenizer and ngramify our dataset, we can see that the prefix padding is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 2]\n",
      "[1, 1, 1, 2, 3]\n",
      "[1, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = NaiveTokenizer(base_vocabulary)\n",
    "dataset = NgramsLanguageModelDataSet(5, data, tokenizer)\n",
    "\n",
    "for ngram in dataset[:5]:\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass it through the dataloader and pad to normalize example lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13, 18, 24, 25, 11],\n",
      "        [60, 61, 62, 63, 57],\n",
      "        [ 1,  1,  1, 36, 18],\n",
      "        [25, 11, 56, 13,  6],\n",
      "        [ 1,  1,  1,  1, 17],\n",
      "        [19, 10, 11, 20, 13],\n",
      "        [ 1,  1,  1,  7, 20],\n",
      "        [38,  9, 10, 11, 39],\n",
      "        [ 9, 10, 11, 12, 13],\n",
      "        [ 1,  1,  1,  1, 57]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True, collate_fn=tokenizer.pad_batch)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.functional import tanh, log_softmax\n",
    "\n",
    "class NNLM (nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size, vocab_size, hidden_size, memory_size, pad_id):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.wordemb = nn.Embedding(vocab_size, emb_size, padding_idx=pad_id)\n",
    "        self.lm_in = nn.Linear(emb_size * memory_size, hidden_size)\n",
    "        self.lm_out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        input_embeddings = self.wordemb(X)\n",
    "        input_embeddings = torch.flatten(input_embeddings, start_dim=1)\n",
    "        hidden_embeddings = self.lm_in(input_embeddings)\n",
    "        logits = self.lm_out(tanh(hidden_embeddings))\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        model_device = next(self.parameters()).device\n",
    "        batch = batch.to(model_device)\n",
    "\n",
    "        X = batch[:,:-1]\n",
    "        Y = batch[:,-1]\n",
    "        logits = log_softmax(self.forward(X), dim=1)\n",
    "        target_logits = torch.gather(logits, 1, Y.unsqueeze(1))\n",
    "        return target_logits, logits\n",
    "\n",
    "\n",
    "    def train(self, dataloader, epochs, device=\"cpu\"): # type: ignore\n",
    "        self.to(device)\n",
    "        cross_entropy = nn.CrossEntropyLoss(ignore_index=self.pad_id)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.005)\n",
    "        epoch_losses = []\n",
    "\n",
    "        pbar = tqdm(range(1, epochs+1))\n",
    "        for epoch in pbar:\n",
    "            loss_list = []\n",
    "            for batch in dataloader:\n",
    "                X =  batch[:,:-1]\n",
    "                Y =  batch[:,-1]\n",
    "                X = X.to(device)\n",
    "                Y = Y.to(device)\n",
    "                logits = self.forward(X)\n",
    "                loss = cross_entropy(logits,Y)\n",
    "                loss.backward()\n",
    "                loss_list.append(loss.item())\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            avg_loss = sum(loss_list) / len(loss_list)\n",
    "            epoch_losses.append(avg_loss)\n",
    "            pbar.set_description(f\"Epoch {epoch}\")\n",
    "            pbar.set_postfix(loss=avg_loss)\n",
    "        return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 100/100 [00:00<00:00, 122.21it/s, loss=0.392]\n"
     ]
    }
   ],
   "source": [
    "lang_model = NNLM(128, tokenizer.vocab_size, 128, 4, tokenizer.pad_id)\n",
    "losses = lang_model.train(dataloader, 100, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test our model on a mock test input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = [\"The Lucky Strike smoker drinks orange juice .\"]\n",
    "test_set = NgramsLanguageModelDataSet(5, test_example, tokenizer)\n",
    "test_loader = DataLoader(test_set, len(test_set), shuffle=False, collate_fn=tokenizer.pad_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  1,  1,  1,  7],\n",
      "        [ 1,  1,  1,  7, 50],\n",
      "        [ 1,  1,  7, 50, 51],\n",
      "        [ 1,  7, 50, 51, 31],\n",
      "        [ 7, 50, 51, 31, 22],\n",
      "        [50, 51, 31, 22, 52],\n",
      "        [51, 31, 22, 52, 53],\n",
      "        [31, 22, 52, 53,  6]])\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "\n",
    "for batch in test_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the surprisal values for each predicted token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lucky Strike smoker drinks orange juice .\n",
      "[('The', [0.26668089628219604]), ('Lucky', [2.749154806137085]), ('Strike', [0.0012287693098187447]), ('smoker', [0.0006437613046728075]), ('drinks', [0.0007314390386454761]), ('orange', [0.0004146431456319988]), ('juice', [0.00056429672986269]), ('.', [0.00017021637177094817])]\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    example = tokenizer.decode_ngram(batch)\n",
    "    surprisals, logits = lang_model(batch)\n",
    "    print(example)\n",
    "    print(list(zip(example.split(),(-(surprisals)).tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax\n",
    "\n",
    "class myLSTM (nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size, vocab_size, hidden_size, pad_id):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.wordemb  = nn.Embedding(vocab_size, emb_size, padding_idx=pad_id)\n",
    "        self.lstm     = nn.LSTM(emb_size, hidden_size, batch_first=True)\n",
    "        self.lm_out   = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def enable_eval_mode(self):\n",
    "        return super().train(False)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        input_embeddings = self.wordemb(X)\n",
    "        hidden_embeddings, _ = self.lstm(input_embeddings)\n",
    "        latest_hidden = hidden_embeddings[:,-1,:]\n",
    "        logits = self.lm_out(latest_hidden)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        model_device = next(self.parameters()).device\n",
    "        batch = batch.to(model_device)\n",
    "        X = batch[:,:-1]\n",
    "        Y = batch[:,-1]\n",
    "        logits = log_softmax(self.forward(X),dim=1)\n",
    "        target_logits = torch.gather(logits, 1, Y.unsqueeze(1))\n",
    "        return target_logits, logits\n",
    "\n",
    "\n",
    "    def train(self, dataloader, epochs, device=\"cpu\"): # type: ignore\n",
    "        self.to(device)\n",
    "        cross_entropy = nn.CrossEntropyLoss(ignore_index=self.pad_id)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.005)\n",
    "        epoch_losses = []\n",
    "\n",
    "        pbar = tqdm(range(1, epochs+1))\n",
    "        for epoch in pbar:\n",
    "            loss_list = []\n",
    "            for batch in dataloader:\n",
    "                X = batch[:,:-1]\n",
    "                Y = batch[:,-1]\n",
    "                X = X.to(device)\n",
    "                Y = Y.to(device)\n",
    "                logits = self.forward(X)\n",
    "                loss = cross_entropy(logits,Y)\n",
    "                loss.backward()\n",
    "                loss_list.append(loss.item())\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            avg_loss = sum(loss_list) / len(loss_list)\n",
    "            epoch_losses.append(avg_loss)\n",
    "            pbar.set_description(f\"Epoch {epoch}\")\n",
    "            pbar.set_postfix(loss=avg_loss)\n",
    "        return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 100/100 [00:01<00:00, 53.76it/s, loss=0.416]\n"
     ]
    }
   ],
   "source": [
    "lang_model = myLSTM(128, tokenizer.vocab_size, 128, tokenizer.pad_id)\n",
    "losses = lang_model.train(dataloader, 100, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the exact same test input as before, we can see how the surprisals change when we use an LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lucky Strike smoker drinks orange juice .\n",
      "[('The', [0.507267415523529]), ('Lucky', [2.672044277191162]), ('Strike', [0.0005697772721759975]), ('smoker', [0.0004401430196594447]), ('drinks', [0.0006123098428361118]), ('orange', [0.000773250067140907]), ('juice', [0.0005809764843434095]), ('.', [0.00013517419574782252])]\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    example = tokenizer.decode_ngram(batch)\n",
    "    surprisals, logits = lang_model(batch)\n",
    "    print(example)\n",
    "    print(list(zip(example.split(),(-(surprisals)).tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal dependencies datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"data/train\", exist_ok=True)\n",
    "os.makedirs(\"data/test\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_languages = [\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_English-GUM/refs/heads/master/en_gum-ud-train.conllu\", \"english\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_German-HDT/refs/heads/master/de_hdt-ud-train-a-1.conllu\", \"german\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Greek-GDT/refs/heads/master/el_gdt-ud-train.conllu\", \"greek\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Polish-MPDT/refs/heads/master/pl_mpdt-ud-train.conllu\", \"polish\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Hungarian-Szeged/refs/heads/master/hu_szeged-ud-train.conllu\", \"hungarian1\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Hungarian-Szeged/refs/heads/dev/hu_szeged-ud-dev.conllu\", \"hungarian2\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Latin-ITTB/refs/heads/master/la_ittb-ud-train.conllu\", \"latin\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Arabic-PADT/refs/heads/master/ar_padt-ud-train.conllu\", \"arabic\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/refs/heads/master/fr_gsd-ud-train.conllu\", \"french\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Turkish-Atis/refs/heads/master/tr_atis-ud-train.conllu\", \"turkish\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSDSimp/refs/heads/master/zh_gsdsimp-ud-train.conllu\", \"mandarin\")\n",
    "]\n",
    "\n",
    "for url, l_name in ud_languages:\n",
    "    outpath = f\"data/train/{l_name}.conllu\"\n",
    "    !wget -q \"{url}\" -O \"{outpath}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhungarian\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datasets\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhungarian1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m train_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhungarian2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhungarian1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhungarian2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "train_datasets[\"hungarian\"] = train_datasets[\"hungarian1\"] + train_datasets[\"hungarian2\"]\n",
    "del train_datasets[\"hungarian1\"]\n",
    "del train_datasets[\"hungarian2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_languages_test = [\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_English-GUM/refs/heads/master/en_gum-ud-test.conllu\", \"english\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_German-HDT/refs/heads/master/de_hdt-ud-test.conllu\", \"german\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Greek-GDT/refs/heads/master/el_gdt-ud-test.conllu\", \"greek\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Polish-MPDT/refs/heads/master/pl_mpdt-ud-test.conllu\", \"polish\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Hungarian-Szeged/refs/heads/master/hu_szeged-ud-test.conllu\", \"hungarian\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Latin-ITTB/refs/heads/master/la_ittb-ud-test.conllu\", \"latin\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Arabic-PADT/refs/heads/master/ar_padt-ud-test.conllu\", \"arabic\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/refs/heads/master/fr_gsd-ud-test.conllu\", \"french\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Turkish-Atis/refs/heads/master/tr_atis-ud-test.conllu\", \"turkish\"),\n",
    "    (\"https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSDSimp/refs/heads/master/zh_gsdsimp-ud-test.conllu\", \"mandarin\")\n",
    "]\n",
    "\n",
    "for url, l_name in ud_languages_test:\n",
    "    outpath = f\"data/test/{l_name}.conllu\"\n",
    "    !wget -q \"{url}\" -O \"{outpath}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the methods we use to format and process our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we do not have the conllu library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to load a dataset from a `.conllu` file. Here we only store the fields that relate to dependencies but we can also store other information like PoS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "\n",
    "def load_conllu(filename):\n",
    "    for sent in conllu.parse_incr(open(filename, \"r\", encoding=\"utf-8\")):\n",
    "        sent = sent.filter(id=lambda x: isinstance(x, int))\n",
    "        ids = [token[\"id\"] for token in sent]\n",
    "        deps = [token[\"head\"] for token in sent]\n",
    "        tokens = [token[\"form\"] for token in sent]\n",
    "        yield ids, deps, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method goes to the relevant path and loads the files in a dictionary where the keys are the different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_from_file(data_dir=\".\"):\n",
    "    datasets = {}\n",
    "    for path in glob.glob(os.path.join(data_dir, \"*.conllu\")):\n",
    "        lang = os.path.splitext(os.path.basename(path))[0]\n",
    "        datasets[lang] = list(load_conllu(path))\n",
    "        print(f\"Loaded {lang}.conllu\")\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will train models on the datasets, we do not want to have datasets of varying lengths as this will bias the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lengths(language_datasets):\n",
    "    for lang, vals in language_datasets.items():\n",
    "\n",
    "        filtered = [\n",
    "            (ids, deps, tokens)\n",
    "            for ids, deps, tokens in vals\n",
    "            if len(tokens) >= 5\n",
    "        ]\n",
    "\n",
    "        if len(filtered) > 2000:\n",
    "            filtered = random.sample(filtered, 2000)\n",
    "\n",
    "        language_datasets[lang] = filtered\n",
    "    return language_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to load dependency trees. For this we have a small `DependencyTree` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "class DependencyTree:\n",
    "\n",
    "    def __init__(self, tokens=None, edges=None):\n",
    "        self.edges: List[Tuple[int, int]]  = [] if edges is None else edges\n",
    "        self.tokens: List[str] = [\"ROOT\"] if tokens is None else tokens\n",
    "\n",
    "\n",
    "    def copy(self):\n",
    "        return DependencyTree(self.tokens[:], self.edges[:])\n",
    "\n",
    "\n",
    "    def N(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "\n",
    "    def sum_dep_length(self):\n",
    "        return sum( abs(dep_idx-gov_idx) for (gov_idx,dep_idx) in self.edges )\n",
    "\n",
    "\n",
    "    def mean_dep_length(self):\n",
    "        return np.mean([abs(dep_idx - gov_idx) for (gov_idx, dep_idx) in self.edges]).item()\n",
    "\n",
    "\n",
    "    @property\n",
    "    def sentence(self):\n",
    "        return self.tokens[1:]\n",
    "\n",
    "\n",
    "    def shuffle_random(self, new_order=None, seed=42):\n",
    "        random.seed(seed)\n",
    "        original_order = list(range(len(self.tokens)))\n",
    "\n",
    "        if new_order:\n",
    "            target_order = new_order\n",
    "            original_order, target_order = target_order, original_order\n",
    "        else:\n",
    "            target_order = original_order[1:]\n",
    "            shuffle(target_order)\n",
    "            target_order = [0] + target_order\n",
    "\n",
    "        dependency_mappings = dict(zip(original_order, target_order))\n",
    "\n",
    "        self.edges = [\n",
    "            (dependency_mappings[gov], dependency_mappings[dep])\n",
    "            for gov, dep in self.edges\n",
    "        ]\n",
    "\n",
    "        new_tokens = [\"NA\"] * len(self.tokens)\n",
    "        new_tokens[0] = self.tokens[0]\n",
    "\n",
    "        for original_order in range(len(self.tokens)):\n",
    "            new_tokens[dependency_mappings[original_order]] = self.tokens[original_order]\n",
    "        self.tokens = new_tokens\n",
    "\n",
    "\n",
    "    def shuffle_projective(self):\n",
    "        children = {}\n",
    "        for gov, dep in self.edges:\n",
    "            deps = children.get(gov, [])\n",
    "            deps.append(dep)\n",
    "            children[gov] = deps\n",
    "\n",
    "        for key in children:\n",
    "            if key == 0:\n",
    "                shuffle(children[key])\n",
    "                children[key] = [0] + children[key]\n",
    "            else:\n",
    "                children[key].append(key)\n",
    "                shuffle(children[key])\n",
    "\n",
    "        stack = set([])\n",
    "        new_order = [0]\n",
    "        while len(new_order) < len(self.tokens):\n",
    "            for idx, el in enumerate(new_order):\n",
    "                if el not in stack:\n",
    "                    stack.add(el)\n",
    "                    left, right = new_order[:idx], new_order[idx+1:]\n",
    "                    new_order = left + children.get(el, [el]) + right\n",
    "                    break\n",
    "\n",
    "        assert(len(new_order) == len(self.tokens))\n",
    "        expected = set(range(len(self.tokens)))\n",
    "        actual = set(new_order)\n",
    "        assert(expected == actual)\n",
    "\n",
    "        self.shuffle_random(new_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different read functions, one reads directly form the `.conllu` file, another form the loaded datasets. The second one is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_v1(data):\n",
    "    for ids, deps, tokens in data:\n",
    "        dep_tree = DependencyTree()\n",
    "\n",
    "        for token in tokens:\n",
    "            dep_tree.tokens.append(token)\n",
    "\n",
    "        for gov, child in zip(deps, ids):\n",
    "            dep_tree.edges.append((int(gov), int(child)))\n",
    "\n",
    "        yield dep_tree\n",
    "\n",
    "\n",
    "def read_v2(stream):\n",
    "    exclude_chars =\".-\"\n",
    "    dep_tree = DependencyTree()\n",
    "\n",
    "    for line in stream:\n",
    "        line = line.strip()\n",
    "        if (line.isspace() or line == \"\" or line.startswith('#')):\n",
    "            if dep_tree.N() > 1:\n",
    "                yield dep_tree\n",
    "                dep_tree = DependencyTree()\n",
    "        else:\n",
    "            idx, word, _, _, _, _, gov_idx, *_  = line.split()\n",
    "            if not any(char in gov_idx for char in exclude_chars) and not any(char in idx for char in exclude_chars):\n",
    "                dep_tree.tokens.append(word)\n",
    "                dep_tree.edges.append((int(gov_idx), int(idx)))\n",
    "\n",
    "    return dep_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_trees(data, links, is_stream=False):\n",
    "    lang_trees = {}\n",
    "\n",
    "    if is_stream:\n",
    "        for _, lang in links:\n",
    "            lang_trees[lang] = list(read_v2(open(f\"{lang}.conllu\", 'r', encoding='utf-8')))\n",
    "\n",
    "    else:\n",
    "        for lang, vals in data.items():\n",
    "            lang_trees[lang] = list(read_v1(vals))\n",
    "\n",
    "    return lang_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will install train and test datasets from universal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
