{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9641d28",
   "metadata": {},
   "source": [
    "---\n",
    "embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca6b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57278c76",
   "metadata": {},
   "source": [
    "First signalling game. Simple, states and messages are integers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4238dc",
   "metadata": {},
   "source": [
    "The scenario is that of a two players signalling game with an environment (The Nature) that selects a state. Given the state, Alice chooses a message and when Bob receives it he selects an action. In case Bob performs the action that matches the state chosen by nature, the cooperation game is a success otherwise it is a failure.\n",
    "\n",
    "The game is characterized by:\n",
    "\n",
    " - the number of states $S$ available to Nature;\n",
    " - the number of messages $M$ available to Alice;\n",
    " - the number of actions $A$ available to Bob.\n",
    "\n",
    "States, messages and actions are represented by vectors of positive or null weights that are to be normalized before sampling. Since the weights are positive, one can normalize them straightforwardly: $$w_i = \\frac{w_i}{\\sum_{j=1}^k w_j}$$ for a vector $\\mathbf{w}$ with length $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b1c7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignallingGame:\n",
    "\n",
    "    def __init__(self, states: int, messages: int, actions: int, seed: int = 42):\n",
    "        self.states = states\n",
    "        self.messages = messages\n",
    "        self.actions = actions\n",
    "        self.message_weights = np.full((states, messages), 1e-6, dtype=float)\n",
    "        self.action_weights = np.full((messages, actions), 1e-6, dtype=float)\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.stats = []\n",
    "\n",
    "\n",
    "    def world_state(self):\n",
    "        return self.rng.randint(self.states)\n",
    "\n",
    "\n",
    "    def emit_message(self, state):\n",
    "        w = self.message_weights[state, :]\n",
    "        probs = w / np.sum(w)\n",
    "        message = self.rng.choice(self.messages, p=probs)\n",
    "        return message\n",
    "\n",
    "\n",
    "    def perform_action(self, message):\n",
    "        w = self.action_weights[message, :]\n",
    "        probs = w / np.sum(w)\n",
    "        action = self.rng.choice(self.actions, p=probs)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def payoff(self, state, action):\n",
    "        return 1 if action == state else 0\n",
    "\n",
    "\n",
    "    def update_weights(self, state, message, action, payoff):\n",
    "        self.message_weights[state, message] += payoff\n",
    "        self.action_weights[message, action] += payoff\n",
    "\n",
    "\n",
    "    def export_weights(self):\n",
    "        return self.message_weights.copy(), self.action_weights.copy()\n",
    "\n",
    "\n",
    "    def snapshot(self, state, message, action, payoff):\n",
    "        mw, aw = self.export_weights()\n",
    "        self.stats.append({\n",
    "            \"s\": state,\n",
    "            \"m\": message,\n",
    "            \"a\": action,\n",
    "            \"p\": payoff,\n",
    "            \"mw\": mw,\n",
    "            \"aw\": aw,\n",
    "        })\n",
    "\n",
    "\n",
    "    def play(self, N: int):\n",
    "        for _ in range(N):\n",
    "            state = self.world_state()\n",
    "            message = self.emit_message(state)\n",
    "            action = self.perform_action(message)\n",
    "            payoff = self.payoff(state, action)\n",
    "            self.update_weights(state, message, action, payoff)\n",
    "            self.snapshot(state, message, action, payoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "92b271fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 1000\n",
      "First 20 payoffs: [0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0]\n",
      "Last 20 payoffs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Total reward: 982\n"
     ]
    }
   ],
   "source": [
    "game1 = SignallingGame(states=3, messages=3, actions=3)\n",
    "game1.play(1000)\n",
    "\n",
    "# Payoffs over time\n",
    "payoffs = [x[\"p\"] for x in game1.stats]\n",
    "print(\"Number of rounds:\", len(payoffs))\n",
    "print(\"First 20 payoffs:\", payoffs[:50])\n",
    "print(\"Last 20 payoffs:\", payoffs[-50:])\n",
    "print(\"Total reward:\", sum(payoffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2824f7",
   "metadata": {},
   "source": [
    "Then I simply add the Roth-Erev update algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d95a0",
   "metadata": {},
   "source": [
    "Implement a simulation method that plays the game $N$ times and updates the player related vectors using the Roth-Erev update. For a single game the update is the following:\n",
    "\n",
    "- $w_i = \\lambda w_i + u$ if $i$ was chosen;\n",
    "- $w_i = \\lambda w_i$ if $i$ was not chosen;\n",
    "- $w_i = \\lambda w_i$ if $i$ was not sampled in this game;\n",
    "\n",
    "where $u$ is the payoff and $\\lambda \\in [0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23b42c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RothErevGame(SignallingGame):\n",
    "    def __init__(self, states: int, messages: int, actions: int, l: float, seed: int = 42):\n",
    "        super().__init__(states, messages, actions, seed)\n",
    "        self.l = l\n",
    "\n",
    "    def update_weights(self, state, message, action, payoff):\n",
    "        self.message_weights[state] *= self.l\n",
    "        self.message_weights[(state, message)] += payoff\n",
    "        self.action_weights[message] *= self.l\n",
    "        self.action_weights[(message, action)] += payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5ec04af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 1000\n",
      "First 20 payoffs: [0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0]\n",
      "Last 20 payoffs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Total reward: 982\n"
     ]
    }
   ],
   "source": [
    "game2 = RothErevGame(states=3, messages=3, actions=3, l=0.5)\n",
    "game2.play(1000)\n",
    "\n",
    "# Payoffs over time\n",
    "payoffs = [x[\"p\"] for x in game2.stats]\n",
    "print(\"Number of rounds:\", len(payoffs))\n",
    "print(\"First 20 payoffs:\", payoffs[:50])\n",
    "print(\"Last 20 payoffs:\", payoffs[-50:])\n",
    "print(\"Total reward:\", sum(payoffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6cf56",
   "metadata": {},
   "source": [
    "Simple update to the original signalling game: just add a minimal version of the reinforce algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf44c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcedGame(SignallingGame):\n",
    "    def __init__(self, states, messages, actions, lr, seed=42):\n",
    "        super().__init__(states, messages, actions, seed)\n",
    "\n",
    "        self.message_weights = torch.nn.Parameter(torch.zeros(states, messages))\n",
    "        self.action_weights = torch.nn.Parameter(torch.zeros(messages, actions))\n",
    "        self.optimizer = torch.optim.SGD([self.message_weights, self.action_weights], lr=lr)\n",
    "\n",
    "\n",
    "    def emit_message(self, state):\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(self.message_weights[state], dim=-1)\n",
    "            message = self.rng.choice(self.messages, p=probs.cpu().numpy())\n",
    "        return message\n",
    "\n",
    "\n",
    "    def perform_action(self, message):\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(self.action_weights[message], dim=-1)\n",
    "            action = self.rng.choice(self.actions, p=probs.cpu().numpy())\n",
    "        return action\n",
    "\n",
    "\n",
    "    def update_weights(self, state, message, action, payoff):\n",
    "        reward = torch.tensor(float(payoff))\n",
    "\n",
    "        # Alice\n",
    "        log_probs_m = torch.log_softmax(self.message_weights[state], dim=-1)[message]\n",
    "        loss_m = -reward * log_probs_m\n",
    "\n",
    "        # Bob\n",
    "        log_probs_a = torch.log_softmax(self.action_weights[message], dim=-1)[action]\n",
    "        loss_a = -reward * log_probs_a\n",
    "\n",
    "        loss = loss_m + loss_a\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def export_weights(self):\n",
    "        return (\n",
    "            self.message_weights.detach().cpu().numpy().copy(),\n",
    "            self.action_weights.detach().cpu().numpy().copy(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "47ff76ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 1000\n",
      "First 20 payoffs: [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0]\n",
      "Last 20 payoffs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Total reward: 950\n"
     ]
    }
   ],
   "source": [
    "game3 = ReinforcedGame(states=3, messages=3, actions=3, lr=1)\n",
    "game3.play(1000)\n",
    "\n",
    "# Payoffs over time\n",
    "payoffs = [x[\"p\"] for x in game3.stats]\n",
    "print(\"Number of rounds:\", len(payoffs))\n",
    "print(\"First 20 payoffs:\", payoffs[:50])\n",
    "print(\"Last 20 payoffs:\", payoffs[-50:])\n",
    "print(\"Total reward:\", sum(payoffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f94b22",
   "metadata": {},
   "source": [
    "Now to implement the reinforce algorithm in the weights update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97934eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcedGameBatch(ReinforcedGame):\n",
    "    def __init__(self, states, messages, actions, lr, batch_size, seed=42):\n",
    "        super().__init__(states, messages, actions, lr, seed)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "    def run_batch(self, batch_n: int):\n",
    "        self.optimizer.zero_grad()\n",
    "        batch_loss = torch.tensor(0.0)\n",
    "\n",
    "        for _ in range(batch_n):\n",
    "            state = self.world_state()\n",
    "            message = self.emit_message(state)\n",
    "            action = self.perform_action(message)\n",
    "            payoff = self.payoff(state, action)\n",
    "\n",
    "            r = torch.tensor(float(payoff))\n",
    "            log_probs_m = torch.log_softmax(self.message_weights[state], dim=-1)[message]\n",
    "            log_probs_a = torch.log_softmax(self.action_weights[message], dim=-1)[action]\n",
    "            batch_loss += -r * (log_probs_m + log_probs_a)\n",
    "            self.snapshot(state, message, action, payoff)\n",
    "\n",
    "        (batch_loss / batch_n).backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def play(self, N: int):\n",
    "        num_batches = N // self.batch_size\n",
    "        remainder = N % self.batch_size\n",
    "\n",
    "        for _ in range(num_batches):\n",
    "            self.run_batch(self.batch_size)\n",
    "\n",
    "        if remainder > 0:\n",
    "            self.run_batch(remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8a7649c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 1000\n",
      "First 20 payoffs: [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0]\n",
      "Last 20 payoffs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Total reward: 950\n"
     ]
    }
   ],
   "source": [
    "game3 = ReinforcedGameBatch(states=3, messages=3, actions=3, lr=1, batch_size=1)\n",
    "game3.play(1000)\n",
    "\n",
    "# Payoffs over time\n",
    "payoffs = [x[\"p\"] for x in game3.stats]\n",
    "print(\"Number of rounds:\", len(payoffs))\n",
    "print(\"First 20 payoffs:\", payoffs[:50])\n",
    "print(\"Last 20 payoffs:\", payoffs[-50:])\n",
    "print(\"Total reward:\", sum(payoffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2cf04e",
   "metadata": {},
   "source": [
    "Now that it works with simple integers, we can try with tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794eb33",
   "metadata": {},
   "source": [
    "The game context is that of Referential games where Nature shows $K$ pictures to Alice and chooses one of them. Then Alice has to send a message to Bob telling which picture she has seen in the list. Bob sees the list of pictures in the same order and has to guess which one Nature had chosen.\n",
    "\n",
    "In our case the pictures will be digits from 0 to 9 coloured in red, green, blue or white and of different sizes: small, medium and large. (You are free to add different properties if you like).\n",
    "\n",
    "We consider that each picture is represented by a tuple $(D,C,S)$ where $D$ is the digit value, $C$ the color and $S$ the size of the digit.\n",
    "\n",
    "The symbolic game is made of 2 agents (Alice and Bob) and one environment (Nature). Nature shows $K$ pictures to Alice and selects one. Alice sends a message to Bob and this is a single symbol taken from a finite set $M$ of messages in this baseline. By design you may wish to structure the symbol as a tuple of 3 features to ease interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52400efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReferentialReinforcedGame:\n",
    "    \"\"\"\n",
    "    A two-agent referential signalling game trained with REINFORCE.\n",
    "\n",
    "    Alice (Sender) observes the target picture and emits a discrete message.\n",
    "    Bob (Receiver) observes the message and all pictures and selects a picture.\n",
    "    Both agents are rewarded if Bob selects the correct target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        K: int,\n",
    "        img_sizes=(10, 4, 3),\n",
    "        lr: float = 0.1,\n",
    "        batch_size: int = 20,\n",
    "        n_digits: int = 10,\n",
    "        n_colors: int = 4,\n",
    "        n_sizes: int = 3,\n",
    "        emb_dim: int = 32,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the game, agents, and learning machinery.\n",
    "        - Fix the structure of the world (number of pictures, feature vocabularies).\n",
    "        - Define Alice's and Bob's internal representations and policies.\n",
    "        - Prepare an optimizer to train both agents jointly via reinforcement learning.\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "        self.img_sizes = img_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.stats = []\n",
    "\n",
    "        self.n_digits = n_digits\n",
    "        self.n_colors = n_colors\n",
    "        self.n_sizes = n_sizes\n",
    "\n",
    "        # --- Alice: policy π_A(m | target(D,C,S)) as 3 categorical heads\n",
    "        # Use embeddings for target features, then linear heads to logits.\n",
    "\n",
    "        # Embeddings for target picture features\n",
    "        self.d_emb = torch.nn.Embedding(n_digits, emb_dim)\n",
    "        self.c_emb = torch.nn.Embedding(n_colors, emb_dim)\n",
    "        self.s_emb = torch.nn.Embedding(n_sizes, emb_dim)\n",
    "\n",
    "        # Message heads: categorical distributions over symbols\n",
    "        self.head_mD = torch.nn.Linear(emb_dim, img_sizes[0])\n",
    "        self.head_mC = torch.nn.Linear(emb_dim, img_sizes[1])\n",
    "        self.head_mS = torch.nn.Linear(emb_dim, img_sizes[2])\n",
    "\n",
    "        # --- Bob: policy π_B(i | m, pictures) over i in {0..K-1}\n",
    "        # Embeddings for received message symbols\n",
    "        self.mD_emb = torch.nn.Embedding(img_sizes[0], emb_dim)\n",
    "        self.mC_emb = torch.nn.Embedding(img_sizes[1], emb_dim)\n",
    "        self.mS_emb = torch.nn.Embedding(img_sizes[2], emb_dim)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    def parameters(self):\n",
    "        return list(self.d_emb.parameters()) + list(self.c_emb.parameters()) + list(self.s_emb.parameters()) + \\\n",
    "               list(self.head_mD.parameters()) + list(self.head_mC.parameters()) + list(self.head_mS.parameters()) + \\\n",
    "               list(self.mD_emb.parameters()) + list(self.mC_emb.parameters()) + list(self.mS_emb.parameters())\n",
    "\n",
    "\n",
    "    # Nature\n",
    "    def world_state(self):\n",
    "        \"\"\"\n",
    "        Sample a world state.\n",
    "        - Nature generates K pictures, each with latent features (D, C, S).\n",
    "        - Nature privately selects one picture as the communicative target.\n",
    "        \"\"\"\n",
    "        pictures = []\n",
    "        for _ in range(self.K):\n",
    "            D = self.rng.randint(self.n_digits)\n",
    "            C = self.rng.randint(self.n_colors)\n",
    "            S = self.rng.randint(self.n_sizes)\n",
    "            pictures.append((D, C, S))\n",
    "        target = self.rng.randint(self.K)\n",
    "        return pictures, target\n",
    "\n",
    "\n",
    "    # Alice\n",
    "    def _target_repr(self, Dt, Ct, St):\n",
    "        \"\"\"\n",
    "        Build Alice's internal representation of the target picture.\n",
    "        - Alice embeds each feature of the target picture.\n",
    "        - The summed embedding represents Alice's private perception.\n",
    "        \"\"\"\n",
    "        Dt = torch.tensor(Dt, dtype=torch.long)\n",
    "        Ct = torch.tensor(Ct, dtype=torch.long)\n",
    "        St = torch.tensor(St, dtype=torch.long)\n",
    "        return self.d_emb(Dt) + self.c_emb(Ct) + self.s_emb(St)\n",
    "\n",
    "\n",
    "    def emit_message(self, pictures, target):\n",
    "        \"\"\"\n",
    "        Alice emits a discrete message describing the target picture.\n",
    "        - Alice observes only the target picture.\n",
    "        - She samples a multi-symbol message from her stochastic policy π_A.\n",
    "        - The message is her attempt to guide Bob to the target.\n",
    "        \"\"\"\n",
    "        Dt, Ct, St = pictures[target]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            h = self._target_repr(Dt, Ct, St)\n",
    "            logits_D = self.head_mD(h)\n",
    "            logits_C = self.head_mC(h)\n",
    "            logits_S = self.head_mS(h)\n",
    "\n",
    "            pD = torch.softmax(logits_D, dim=-1).cpu().numpy()\n",
    "            pC = torch.softmax(logits_C, dim=-1).cpu().numpy()\n",
    "            pS = torch.softmax(logits_S, dim=-1).cpu().numpy()\n",
    "\n",
    "        mD = self.rng.choice(self.img_sizes[0], p=pD)\n",
    "        mC = self.rng.choice(self.img_sizes[1], p=pC)\n",
    "        mS = self.rng.choice(self.img_sizes[2], p=pS)\n",
    "\n",
    "        message = (mD, mC, mS)\n",
    "        return message\n",
    "\n",
    "\n",
    "    # Bob\n",
    "    def _message_repr(self, m):\n",
    "        \"\"\"\n",
    "        Build Bob's internal representation of the received message.\n",
    "        - Bob embeds each symbol of Alice's message.\n",
    "        - The combined embedding represents Bob's interpretation.\n",
    "        \"\"\"\n",
    "        mD, mC, mS = m\n",
    "        mD = torch.tensor(mD, dtype=torch.long)\n",
    "        mC = torch.tensor(mC, dtype=torch.long)\n",
    "        mS = torch.tensor(mS, dtype=torch.long)\n",
    "        return self.mD_emb(mD) + self.mC_emb(mC) + self.mS_emb(mS)\n",
    "\n",
    "\n",
    "    def _picture_repr(self, pic):\n",
    "        \"\"\"\n",
    "        Build Bob's internal representation of a picture.\n",
    "        - Bob embeds each candidate picture.\n",
    "        \"\"\"\n",
    "        D, C, S = pic\n",
    "        D = torch.tensor(D, dtype=torch.long)\n",
    "        C = torch.tensor(C, dtype=torch.long)\n",
    "        S = torch.tensor(S, dtype=torch.long)\n",
    "        return self.d_emb(D) + self.c_emb(C) + self.s_emb(S)\n",
    "\n",
    "\n",
    "    def perform_action(self, pictures, message):\n",
    "        \"\"\"\n",
    "        Bob selects a picture based on the received message.\n",
    "        - Bob scores each picture by similarity with the message.\n",
    "        - He samples a picture index from his policy π_B.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            message_vec = self._message_repr(message)\n",
    "            scores = []\n",
    "            for pic in pictures:\n",
    "                pic_vec = self._picture_repr(pic)\n",
    "                scores.append(torch.dot(pic_vec, message_vec))\n",
    "            scores = torch.stack(scores) # (K,)\n",
    "            probs = torch.softmax(scores, dim=-1).cpu().numpy()\n",
    "        return self.rng.choice(self.K, p=probs)\n",
    "\n",
    "\n",
    "    def payoff(self, target, action):\n",
    "        \"\"\"Compute the shared reward.\"\"\"\n",
    "        return 1 if action == target else 0\n",
    "\n",
    "\n",
    "    # Reinforce over batches\n",
    "    def run_batch(self, batch_n: int):\n",
    "        \"\"\"Run a batch of episodes and update both agents via REINFORCE.\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        batch_loss = torch.zeros(())\n",
    "\n",
    "        for _ in range(batch_n):\n",
    "            pictures, target = self.world_state()\n",
    "            message = self.emit_message(pictures, target)\n",
    "            action = self.perform_action(pictures, message)\n",
    "            r = float(self.payoff(target, action))\n",
    "            reward = torch.tensor(r, dtype=torch.float32)\n",
    "\n",
    "            # log π_A(m | target)\n",
    "            Dt, Ct, St = pictures[target]\n",
    "            h = self._target_repr(Dt, Ct, St)\n",
    "            log_probs_mD = F.log_softmax(self.head_mD(h), dim=-1)[message[0]]\n",
    "            log_probs_mC = F.log_softmax(self.head_mC(h), dim=-1)[message[1]]\n",
    "            log_probs_mS = F.log_softmax(self.head_mS(h), dim=-1)[message[2]]\n",
    "            log_probs_A = log_probs_mD + log_probs_mC + log_probs_mS\n",
    "\n",
    "            # log π_B(a | m, pictures)\n",
    "            message_vec = self._message_repr(message)\n",
    "            scores = []\n",
    "            for pic in pictures:\n",
    "                pic_vec = self._picture_repr(pic)\n",
    "                scores.append(torch.dot(pic_vec, message_vec))\n",
    "            scores = torch.stack(scores)\n",
    "            log_probs_B = F.log_softmax(scores, dim=-1)[action]\n",
    "\n",
    "            # Reinforce loss: -r * (log π_A + log π_B)\n",
    "            batch_loss = batch_loss + (-reward * (log_probs_A + log_probs_B))\n",
    "\n",
    "            self.stats.append({\"pics\": pictures, \"t\": target, \"m\": message, \"a\": action, \"p\": int(r)})\n",
    "\n",
    "        (batch_loss / batch_n).backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def play(self, N: int):\n",
    "        \"\"\"Run N episodes of the signalling game.\"\"\"\n",
    "        num_batches = N // self.batch_size\n",
    "        rem = N % self.batch_size\n",
    "\n",
    "        for _ in range(num_batches):\n",
    "            self.run_batch(self.batch_size)\n",
    "        \n",
    "        if rem > 0:\n",
    "            self.run_batch(rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d08f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 10000\n",
      "First 50 payoffs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Last 50 payoffs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "Total reward: 7415\n"
     ]
    }
   ],
   "source": [
    "game4 = ReferentialReinforcedGame(K=5, lr=0.7, batch_size=20)\n",
    "game4.play(10000)\n",
    "\n",
    "payoffs = [x[\"p\"] for x in game4.stats]\n",
    "print(\"Number of rounds:\", len(payoffs))\n",
    "print(\"First 50 payoffs:\", payoffs[:50])\n",
    "print(\"Last 50 payoffs:\", payoffs[-50:])\n",
    "print(\"Total reward:\", sum(payoffs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
