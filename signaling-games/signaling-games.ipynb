{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9641d28",
   "metadata": {},
   "source": [
    "---\n",
    "embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aca6b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57278c76",
   "metadata": {},
   "source": [
    "First signalling game. Simple, states and messages are integers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4238dc",
   "metadata": {},
   "source": [
    "The scenario is that of a two players signalling game with an environment (The Nature) that selects a state. Given the state, Alice chooses a message and when Bob receives it he selects an action. In case Bob performs the action that matches the state chosen by nature, the cooperation game is a success otherwise it is a failure.\n",
    "\n",
    "The game is characterized by:\n",
    "\n",
    " - the number of states $S$ available to Nature;\n",
    " - the number of messages $M$ available to Alice;\n",
    " - the number of actions $A$ available to Bob.\n",
    "\n",
    "States, messages and actions are represented by vectors of positive or null weights that are to be normalized before sampling. Since the weights are positive, one can normalize them straightforwardly: $$w_i = \\frac{w_i}{\\sum_{j=1}^k w_j}$$ for a vector $\\mathbf{w}$ with length $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1c7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignallingGame:\n",
    "\n",
    "    def __init__(self, states: int, messages: int, actions: int, seed: int = 42):\n",
    "        self.states = states\n",
    "        self.messages = messages\n",
    "        self.actions = actions\n",
    "        self.message_weights = np.full((states, messages), 1e-6, dtype=float)\n",
    "        self.action_weights = np.full((messages, actions), 1e-6, dtype=float)\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.stats = []\n",
    "\n",
    "\n",
    "    def world_state(self):\n",
    "        return self.rng.randint(self.states)\n",
    "\n",
    "\n",
    "    def emit_message(self, state):\n",
    "        w = self.message_weights[state, :]\n",
    "        probs = w / np.sum(w)\n",
    "        message = self.rng.choice(self.messages, p=probs)\n",
    "        return message\n",
    "\n",
    "\n",
    "    def perform_action(self, message):\n",
    "        w = self.action_weights[message, :]\n",
    "        probs = w / np.sum(w)\n",
    "        action = self.rng.choice(self.actions, p=probs)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def payoff(self, state, action):\n",
    "        return 1 if action == state else 0\n",
    "\n",
    "\n",
    "    def update_weights(self, state, message, action, payoff):\n",
    "        self.message_weights[state, message] += payoff\n",
    "        self.action_weights[message, action] += payoff\n",
    "\n",
    "\n",
    "    def export_weights(self):\n",
    "        return self.message_weights.copy(), self.action_weights.copy()\n",
    "\n",
    "\n",
    "    def snapshot(self, state, message, action, payoff):\n",
    "        mw, aw = self.export_weights()\n",
    "        self.stats.append({\n",
    "            \"s\": state,\n",
    "            \"m\": message,\n",
    "            \"a\": action,\n",
    "            \"p\": payoff,\n",
    "            \"mw\": mw,\n",
    "            \"aw\": aw,\n",
    "        })\n",
    "\n",
    "\n",
    "    def play(self, N: int):\n",
    "        for _ in range(N):\n",
    "            state = self.world_state()\n",
    "            message = self.emit_message(state)\n",
    "            action = self.perform_action(message)\n",
    "            payoff = self.payoff(state, action)\n",
    "            self.update_weights(state, message, action, payoff)\n",
    "            self.snapshot(state, message, action, payoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92b271fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 1000\n",
      "First 20 payoffs: [0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0]\n",
      "Last 20 payoffs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Success rate: 98.20%\n"
     ]
    }
   ],
   "source": [
    "game1 = SignallingGame(states=3, messages=3, actions=3)\n",
    "game1.play(1000)\n",
    "\n",
    "# Payoffs over time\n",
    "payoffs = [x[\"p\"] for x in game1.stats]\n",
    "print(\"Number of rounds:\", len(payoffs))\n",
    "print(\"First 20 payoffs:\", payoffs[:50])\n",
    "print(\"Last 20 payoffs:\", payoffs[-50:])\n",
    "print(f\"Success rate: {sum(payoffs)/len(payoffs)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2824f7",
   "metadata": {},
   "source": [
    "Then I simply add the Roth-Erev update algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d95a0",
   "metadata": {},
   "source": [
    "Implement a simulation method that plays the game $N$ times and updates the player related vectors using the Roth-Erev update. For a single game the update is the following:\n",
    "\n",
    "- $w_i = \\lambda w_i + u$ if $i$ was chosen;\n",
    "- $w_i = \\lambda w_i$ if $i$ was not chosen;\n",
    "- $w_i = \\lambda w_i$ if $i$ was not sampled in this game;\n",
    "\n",
    "where $u$ is the payoff and $\\lambda \\in [0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23b42c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RothErevGame(SignallingGame):\n",
    "    def __init__(self, states: int, messages: int, actions: int, l: float, seed: int = 42):\n",
    "        super().__init__(states, messages, actions, seed)\n",
    "        self.l = l\n",
    "\n",
    "    def update_weights(self, state, message, action, payoff):\n",
    "        self.message_weights[state] *= self.l\n",
    "        self.message_weights[(state, message)] += payoff\n",
    "        self.action_weights[message] *= self.l\n",
    "        self.action_weights[(message, action)] += payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5ec04af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 1000\n",
      "First 20 payoffs: [0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0]\n",
      "Last 20 payoffs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Success rate: 98.20%\n"
     ]
    }
   ],
   "source": [
    "game2 = RothErevGame(states=3, messages=3, actions=3, l=0.5)\n",
    "game2.play(1000)\n",
    "\n",
    "# Payoffs over time\n",
    "payoffs = [x[\"p\"] for x in game2.stats]\n",
    "print(\"Number of rounds:\", len(payoffs))\n",
    "print(\"First 20 payoffs:\", payoffs[:50])\n",
    "print(\"Last 20 payoffs:\", payoffs[-50:])\n",
    "print(f\"Success rate: {sum(payoffs)/len(payoffs)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6cf56",
   "metadata": {},
   "source": [
    "Simple update to the original signalling game: just add a minimal version of the reinforce algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf44c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcedGame(SignallingGame):\n",
    "    def __init__(self, states, messages, actions, lr, seed=42):\n",
    "        super().__init__(states, messages, actions, seed)\n",
    "\n",
    "        self.message_weights = torch.nn.Parameter(torch.zeros(states, messages))\n",
    "        self.action_weights = torch.nn.Parameter(torch.zeros(messages, actions))\n",
    "        self.optimizer = torch.optim.SGD([self.message_weights, self.action_weights], lr=lr)\n",
    "\n",
    "\n",
    "    def emit_message(self, state):\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(self.message_weights[state], dim=-1)\n",
    "            message = self.rng.choice(self.messages, p=probs.cpu().numpy())\n",
    "        return message\n",
    "\n",
    "\n",
    "    def perform_action(self, message):\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(self.action_weights[message], dim=-1)\n",
    "            action = self.rng.choice(self.actions, p=probs.cpu().numpy())\n",
    "        return action\n",
    "\n",
    "\n",
    "    def update_weights(self, state, message, action, payoff):\n",
    "        reward = torch.tensor(float(payoff))\n",
    "\n",
    "        # Alice\n",
    "        log_probs_m = torch.log_softmax(self.message_weights[state], dim=-1)[message]\n",
    "        loss_m = -reward * log_probs_m\n",
    "\n",
    "        # Bob\n",
    "        log_probs_a = torch.log_softmax(self.action_weights[message], dim=-1)[action]\n",
    "        loss_a = -reward * log_probs_a\n",
    "\n",
    "        loss = loss_m + loss_a\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def export_weights(self):\n",
    "        return (\n",
    "            self.message_weights.detach().cpu().numpy().copy(),\n",
    "            self.action_weights.detach().cpu().numpy().copy(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47ff76ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 1000\n",
      "First 20 payoffs: [0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0]\n",
      "Last 20 payoffs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Success rate: 92.00%\n"
     ]
    }
   ],
   "source": [
    "game3 = ReinforcedGame(states=3, messages=3, actions=3, lr=0.5)\n",
    "game3.play(1000)\n",
    "\n",
    "# Payoffs over time\n",
    "payoffs = [x[\"p\"] for x in game3.stats]\n",
    "print(\"Number of rounds:\", len(payoffs))\n",
    "print(\"First 20 payoffs:\", payoffs[:50])\n",
    "print(\"Last 20 payoffs:\", payoffs[-50:])\n",
    "print(f\"Success rate: {sum(payoffs)/len(payoffs)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f94b22",
   "metadata": {},
   "source": [
    "Now to implement the reinforce algorithm in the weights update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d97934eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcedGameBatch(ReinforcedGame):\n",
    "    def __init__(self, states, messages, actions, lr, batch_size, seed=42):\n",
    "        super().__init__(states, messages, actions, lr, seed)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "    def run_batch(self, batch_n: int):\n",
    "        self.optimizer.zero_grad()\n",
    "        batch_loss = torch.tensor(0.0)\n",
    "\n",
    "        for _ in range(batch_n):\n",
    "            state = self.world_state()\n",
    "            message = self.emit_message(state)\n",
    "            action = self.perform_action(message)\n",
    "            payoff = self.payoff(state, action)\n",
    "\n",
    "            r = torch.tensor(float(payoff))\n",
    "            log_probs_m = torch.log_softmax(self.message_weights[state], dim=-1)[message]\n",
    "            log_probs_a = torch.log_softmax(self.action_weights[message], dim=-1)[action]\n",
    "            batch_loss += -r * (log_probs_m + log_probs_a)\n",
    "            self.snapshot(state, message, action, payoff)\n",
    "\n",
    "        (batch_loss / batch_n).backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def play(self, N: int):\n",
    "        num_batches = N // self.batch_size\n",
    "        remainder = N % self.batch_size\n",
    "\n",
    "        for _ in range(num_batches):\n",
    "            self.run_batch(self.batch_size)\n",
    "\n",
    "        if remainder > 0:\n",
    "            self.run_batch(remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a7649c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 1000\n",
      "First 20 payoffs: [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0]\n",
      "Last 20 payoffs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Total reward: 950\n"
     ]
    }
   ],
   "source": [
    "game3 = ReinforcedGameBatch(states=3, messages=3, actions=3, lr=1, batch_size=1)\n",
    "game3.play(1000)\n",
    "\n",
    "# Payoffs over time\n",
    "payoffs = [x[\"p\"] for x in game3.stats]\n",
    "print(\"Number of rounds:\", len(payoffs))\n",
    "print(\"First 20 payoffs:\", payoffs[:50])\n",
    "print(\"Last 20 payoffs:\", payoffs[-50:])\n",
    "print(\"Total reward:\", sum(payoffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2cf04e",
   "metadata": {},
   "source": [
    "Now that it works with simple integers, we can try with tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794eb33",
   "metadata": {},
   "source": [
    "The game context is that of Referential games where Nature shows $K$ pictures to Alice and chooses one of them. Then Alice has to send a message to Bob telling which picture she has seen in the list. Bob sees the list of pictures in the same order and has to guess which one Nature had chosen.\n",
    "\n",
    "In our case the pictures will be digits from 0 to 9 coloured in red, green, blue or white and of different sizes: small, medium and large. (You are free to add different properties if you like).\n",
    "\n",
    "We consider that each picture is represented by a tuple $(D,C,S)$ where $D$ is the digit value, $C$ the color and $S$ the size of the digit.\n",
    "\n",
    "The symbolic game is made of 2 agents (Alice and Bob) and one environment (Nature). Nature shows $K$ pictures to Alice and selects one. Alice sends a message to Bob and this is a single symbol taken from a finite set $M$ of messages in this baseline. By design you may wish to structure the symbol as a tuple of 3 features to ease interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52400efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReferentialReinforcedGame:\n",
    "    \"\"\"\n",
    "    A two-agent referential signalling game trained with REINFORCE.\n",
    "\n",
    "    Alice (Sender) observes the target picture and emits a discrete message.\n",
    "    Bob (Receiver) observes the message and all pictures and selects a picture.\n",
    "    Both agents are rewarded if Bob selects the correct target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        K: int,\n",
    "        img_sizes=(10, 4, 3),\n",
    "        lr: float = 0.1,\n",
    "        batch_size: int = 20,\n",
    "        n_digits: int = 10,\n",
    "        n_colors: int = 4,\n",
    "        n_sizes: int = 3,\n",
    "        emb_dim: int = 3,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the game, agents, and learning machinery.\n",
    "        - Fix the structure of the world (number of pictures, feature vocabularies).\n",
    "        - Define Alice's and Bob's internal representations and policies.\n",
    "        - Prepare an optimizer to train both agents jointly via reinforcement learning.\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "        self.img_sizes = img_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.stats = []\n",
    "\n",
    "        self.n_digits = img_sizes[0]\n",
    "        self.n_colors = img_sizes[1]\n",
    "        self.n_sizes = img_sizes[2]\n",
    "\n",
    "        # --- Alice: policy π_A(m | target(D,C,S)) as 3 categorical heads\n",
    "        # Use embeddings for target features, then linear heads to logits.\n",
    "\n",
    "        # Embeddings for target picture features\n",
    "        self.d_emb = torch.nn.Embedding(img_sizes[0], emb_dim)\n",
    "        self.c_emb = torch.nn.Embedding(img_sizes[1], emb_dim)\n",
    "        self.s_emb = torch.nn.Embedding(img_sizes[2], emb_dim)\n",
    "\n",
    "        # Message heads: categorical distributions over symbols\n",
    "        self.head_mD = torch.nn.Linear(emb_dim, img_sizes[0])\n",
    "        self.head_mC = torch.nn.Linear(emb_dim, img_sizes[1])\n",
    "        self.head_mS = torch.nn.Linear(emb_dim, img_sizes[2])\n",
    "\n",
    "        # --- Bob: policy π_B(i | m, pictures) over i in {0..K-1}\n",
    "        # Embeddings for received message symbols\n",
    "        self.mD_emb = torch.nn.Embedding(img_sizes[0], emb_dim)\n",
    "        self.mC_emb = torch.nn.Embedding(img_sizes[1], emb_dim)\n",
    "        self.mS_emb = torch.nn.Embedding(img_sizes[2], emb_dim)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    def parameters(self):\n",
    "        return list(self.d_emb.parameters()) + list(self.c_emb.parameters()) + list(self.s_emb.parameters()) + \\\n",
    "               list(self.head_mD.parameters()) + list(self.head_mC.parameters()) + list(self.head_mS.parameters()) + \\\n",
    "               list(self.mD_emb.parameters()) + list(self.mC_emb.parameters()) + list(self.mS_emb.parameters())\n",
    "\n",
    "\n",
    "    # Nature\n",
    "    def world_state(self):\n",
    "        \"\"\"\n",
    "        Sample a world state.\n",
    "        - Nature generates K pictures, each with latent features (D, C, S).\n",
    "        - Nature privately selects one picture as the communicative target.\n",
    "        \"\"\"\n",
    "        pictures = []\n",
    "        for _ in range(self.K):\n",
    "            D = self.rng.randint(self.n_digits)\n",
    "            C = self.rng.randint(self.n_colors)\n",
    "            S = self.rng.randint(self.n_sizes)\n",
    "            pictures.append((D, C, S))\n",
    "        target = self.rng.randint(self.K)\n",
    "        return pictures, target\n",
    "\n",
    "\n",
    "    # Alice\n",
    "    def _target_repr(self, Dt, Ct, St):\n",
    "        \"\"\"\n",
    "        Build Alice's internal representation of the target picture.\n",
    "        - Alice embeds each feature of the target picture.\n",
    "        - The summed embedding represents Alice's private perception.\n",
    "        \"\"\"\n",
    "        Dt = torch.tensor(Dt, dtype=torch.long)\n",
    "        Ct = torch.tensor(Ct, dtype=torch.long)\n",
    "        St = torch.tensor(St, dtype=torch.long)\n",
    "        return self.d_emb(Dt) + self.c_emb(Ct) + self.s_emb(St)\n",
    "\n",
    "\n",
    "    def emit_message(self, pictures, target):\n",
    "        \"\"\"\n",
    "        Alice emits a discrete message describing the target picture.\n",
    "        - Alice observes only the target picture.\n",
    "        - She samples a multi-symbol message from her stochastic policy π_A.\n",
    "        - The message is her attempt to guide Bob to the target.\n",
    "        \"\"\"\n",
    "        Dt, Ct, St = pictures[target]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            h = self._target_repr(Dt, Ct, St)\n",
    "            logits_D = self.head_mD(h)\n",
    "            logits_C = self.head_mC(h)\n",
    "            logits_S = self.head_mS(h)\n",
    "\n",
    "            pD = torch.softmax(logits_D, dim=-1).cpu().numpy()\n",
    "            pC = torch.softmax(logits_C, dim=-1).cpu().numpy()\n",
    "            pS = torch.softmax(logits_S, dim=-1).cpu().numpy()\n",
    "\n",
    "        mD = self.rng.choice(self.img_sizes[0], p=pD)\n",
    "        mC = self.rng.choice(self.img_sizes[1], p=pC)\n",
    "        mS = self.rng.choice(self.img_sizes[2], p=pS)\n",
    "\n",
    "        message = (mD, mC, mS)\n",
    "        return message\n",
    "\n",
    "\n",
    "    # Bob\n",
    "    def _message_repr(self, m):\n",
    "        \"\"\"\n",
    "        Build Bob's internal representation of the received message.\n",
    "        - Bob embeds each symbol of Alice's message.\n",
    "        - The combined embedding represents Bob's interpretation.\n",
    "        \"\"\"\n",
    "        mD, mC, mS = m\n",
    "        mD = torch.tensor(mD, dtype=torch.long)\n",
    "        mC = torch.tensor(mC, dtype=torch.long)\n",
    "        mS = torch.tensor(mS, dtype=torch.long)\n",
    "        return self.mD_emb(mD) + self.mC_emb(mC) + self.mS_emb(mS)\n",
    "\n",
    "\n",
    "    def _picture_repr(self, pic):\n",
    "        \"\"\"\n",
    "        Build Bob's internal representation of a picture.\n",
    "        - Bob embeds each candidate picture.\n",
    "        \"\"\"\n",
    "        D, C, S = pic\n",
    "        D = torch.tensor(D, dtype=torch.long)\n",
    "        C = torch.tensor(C, dtype=torch.long)\n",
    "        S = torch.tensor(S, dtype=torch.long)\n",
    "        return self.d_emb(D) + self.c_emb(C) + self.s_emb(S)\n",
    "\n",
    "\n",
    "    def perform_action(self, pictures, message):\n",
    "        \"\"\"\n",
    "        Bob selects a picture based on the received message.\n",
    "        - Bob scores each picture by similarity with the message.\n",
    "        - He samples a picture index from his policy π_B.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            message_vec = self._message_repr(message)\n",
    "            scores = []\n",
    "            for pic in pictures:\n",
    "                pic_vec = self._picture_repr(pic)\n",
    "                scores.append(torch.dot(pic_vec, message_vec))\n",
    "            scores = torch.stack(scores) # (K,)\n",
    "            probs = torch.softmax(scores, dim=-1).cpu().numpy()\n",
    "        return self.rng.choice(self.K, p=probs)\n",
    "\n",
    "\n",
    "    def payoff(self, target, action):\n",
    "        \"\"\"Compute the shared reward.\"\"\"\n",
    "        return 1 if action == target else 0\n",
    "\n",
    "\n",
    "    # Reinforce over batches\n",
    "    def run_batch(self, batch_n: int):\n",
    "        \"\"\"Run a batch of episodes and update both agents via REINFORCE.\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        batch_loss = torch.zeros(())\n",
    "\n",
    "        for _ in range(batch_n):\n",
    "            pictures, target = self.world_state()\n",
    "            message = self.emit_message(pictures, target)\n",
    "            action = self.perform_action(pictures, message)\n",
    "            r = float(self.payoff(target, action))\n",
    "            reward = torch.tensor(r, dtype=torch.float32)\n",
    "\n",
    "            # log π_A(m | target)\n",
    "            Dt, Ct, St = pictures[target]\n",
    "            h = self._target_repr(Dt, Ct, St)\n",
    "            log_probs_mD = F.log_softmax(self.head_mD(h), dim=-1)[message[0]]\n",
    "            log_probs_mC = F.log_softmax(self.head_mC(h), dim=-1)[message[1]]\n",
    "            log_probs_mS = F.log_softmax(self.head_mS(h), dim=-1)[message[2]]\n",
    "            log_probs_A = log_probs_mD + log_probs_mC + log_probs_mS\n",
    "\n",
    "            # log π_B(a | m, pictures)\n",
    "            message_vec = self._message_repr(message)\n",
    "            scores = []\n",
    "            for pic in pictures:\n",
    "                pic_vec = self._picture_repr(pic)\n",
    "                scores.append(torch.dot(pic_vec, message_vec))\n",
    "            scores = torch.stack(scores)\n",
    "            log_probs_B = F.log_softmax(scores, dim=-1)[action]\n",
    "\n",
    "            # Reinforce loss: -r * (log π_A + log π_B)\n",
    "            batch_loss = batch_loss + (-reward * (log_probs_A + log_probs_B))\n",
    "\n",
    "            self.stats.append({\"pics\": pictures, \"t\": target, \"m\": message, \"a\": action, \"p\": int(r)})\n",
    "\n",
    "        (batch_loss / batch_n).backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def play(self, N: int):\n",
    "        \"\"\"Run N episodes of the signalling game.\"\"\"\n",
    "        num_batches = N // self.batch_size\n",
    "        rem = N % self.batch_size\n",
    "\n",
    "        for _ in range(num_batches):\n",
    "            self.run_batch(self.batch_size)\n",
    "        \n",
    "        if rem > 0:\n",
    "            self.run_batch(rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d08f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 20000\n",
      "First 50 payoffs: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Last 50 payoffs: [1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "Success rate: 66.88%\n"
     ]
    }
   ],
   "source": [
    "game4 = ReferentialReinforcedGame(K=5, lr=0.5, batch_size=20)\n",
    "game4.play(20000)\n",
    "\n",
    "payoffs = [x[\"p\"] for x in game4.stats]\n",
    "print(\"Number of rounds:\", len(payoffs))\n",
    "print(\"First 50 payoffs:\", payoffs[:50])\n",
    "print(\"Last 50 payoffs:\", payoffs[-50:])\n",
    "print(f\"Success rate: {sum(payoffs)/len(payoffs)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d635705c",
   "metadata": {},
   "source": [
    "Try the signalling game with real images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da0b99c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example: (<PIL.Image.Image image mode=L size=28x28 at 0x157445C40>, 5), an object of <class 'tuple'>.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "dataset = MNIST(\"data\", download=True)\n",
    "print(f\"First example: {dataset[0]}, an object of {type(dataset[0])}.\")\n",
    "plt.imshow(dataset[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c749fc8",
   "metadata": {},
   "source": [
    "Turn the image to a standard torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d1a1e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "\n",
    "img_tensor = pil_to_tensor(dataset[0][0])\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e891b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Pad, Resize\n",
    "\n",
    "\n",
    "def scale_transform(image_tensor, scale_code):\n",
    "    \"\"\"\n",
    "    Scale image by padding and resizing.\n",
    "    scale_code: 0 for original size, 1 for small, 2 for very small\n",
    "    \"\"\"\n",
    "    padded = Pad(10 * scale_code, fill=0)(image_tensor)\n",
    "    scaled = Resize((28, 28))(padded)\n",
    "    return scaled\n",
    "\n",
    "\n",
    "def color_transform(image_tensor, color_code):\n",
    "    \"\"\"\n",
    "    Convert grayscale image to coloured RGB image.\n",
    "    color_code: Color channel - \"red\", \"green\", or \"blue\"\n",
    "    \"\"\"\n",
    "    coloured = torch.zeros((3, 28, 28), dtype=torch.float32)\n",
    "    \n",
    "    color_channels = {\n",
    "        \"red\": 0,\n",
    "        \"green\": 1,\n",
    "        \"blue\": 2\n",
    "    }\n",
    "    \n",
    "    if color_code not in color_channels:\n",
    "        raise ValueError(f\"Invalid color code: {color_code}. Must be 'red', 'green', or 'blue'\")\n",
    "    \n",
    "    coloured[color_channels[color_code]] = image_tensor\n",
    "    return coloured\n",
    "\n",
    "\n",
    "def color_scale(image_tensor, color_code=\"green\", scale_code=0):\n",
    "    \"\"\"\n",
    "    Apply color transformation followed by scaling.\n",
    "    color_code: Color channel - \"red\", \"green\", or \"blue\" (default: \"green\")\n",
    "    scale_code: 0 for original size, 1 for small, 2 for very small (default: 0)\n",
    "    \"\"\"\n",
    "    return scale_transform(color_transform(image_tensor, color_code), scale_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2468ea5c",
   "metadata": {},
   "source": [
    "Create a randomized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dedb71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Configuration\n",
    "COLORS = [\"red\", \"green\", \"blue\"]\n",
    "SIZES = [0, 1, 2]\n",
    "\n",
    "def randomize_tensorize_img(img_label_pair):\n",
    "    img = ToTensor()(img_label_pair[0])\n",
    "    color = random.choice(COLORS)\n",
    "    size = random.choice(SIZES)\n",
    "    return color_scale(img, color_code=color, scale_code=size)\n",
    "\n",
    "# Transform entire dataset: random colors and sizes\n",
    "mnist_colors = torch.stack([randomize_tensorize_img(item) for item in dataset])\n",
    "\n",
    "# Extract labels\n",
    "gold_classes = [label for _, label in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49646a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_colors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d698e5d",
   "metadata": {},
   "source": [
    "Now to build an auto-encoder.\n",
    "\n",
    "To get an image vector, we will use a primitive method for encoding the images. First we get a raw vector by flattening the image matrices (one matrix for each channel) and then these three vectors are concatenated.\n",
    "\n",
    "Then we perform dimension reduction by using a simple auto-encoder. An auto-encoder is a neural network whose task it to predict its input. The key feature is that the auto-encoder maps the input to a low dimension vector and then maps this low dimension representation again to the input dimension. In its simplest form it takes the form of a feed-forward network trained with an MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d9b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "\n",
    "\n",
    "class MNISTAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder for MNIST images with color channels.\n",
    "    Encodes flattened RGB images (3*28*28 = 2352 dims) to a lower dimensional\n",
    "    latent representation and reconstructs the original image.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, input_dim=2352, latent_dim=64):\n",
    "        \"\"\"\n",
    "        input_dim: Flattened image dimension (default: 28*28*3 = 2352)\n",
    "        latent_dim: Dimension of latent representation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder: input_dim -> 128 -> latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "\n",
    "        # Decoder: latent_dim -> 128 -> input_dim\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()  # Output in [0, 1] range\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "    def encode(self, image):\n",
    "        \"\"\"\n",
    "        image: Tensor of shape (3, 28, 28) or batch (N, 3, 28, 28)\n",
    "        Returns: latent representation tensor\n",
    "        \"\"\"\n",
    "        # Handle single image or batch\n",
    "        if image.dim() == 3:\n",
    "            image = image.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Flatten to (batch_size, input_dim)\n",
    "        flattened = image.reshape(-1, self.input_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoded = self.encoder(flattened)\n",
    "        \n",
    "        return encoded.squeeze(0) if encoded.size(0) == 1 else encoded\n",
    "\n",
    "\n",
    "    def train_autoencoder(self, dataset, batch_size=64, epochs=10, lr=1e-3, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        dataset: Tensor of images, shape (N, 3, 28, 28)\n",
    "        batch_size: Number of samples per batch\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        device: Device to train on ('cpu' or 'cuda')\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        n_batches = len(dataset) // batch_size\n",
    "\n",
    "        # Progress bar for batches\n",
    "        progress_bar = tqdm(range(epochs), desc=f\"Training over {epochs} epochs\")\n",
    "\n",
    "        for _ in progress_bar:\n",
    "            self.train()  # Set to training mode\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for batch_idx in range(n_batches):\n",
    "                # Get batch and flatten\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                batch = dataset[start_idx:end_idx]\n",
    "                x = batch.reshape(-1, self.input_dim).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                x_reconstructed = self.forward(x)\n",
    "                loss = criterion(x_reconstructed, x)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "\n",
    "            # Print epoch statistics\n",
    "            avg_loss = total_loss / (n_batches * batch_size)\n",
    "            progress_bar.set_postfix({'loss': f'{avg_loss:.6f}'})\n",
    "\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "\n",
    "\n",
    "    def load_model(self, filename, device=\"cpu\"):\n",
    "        self.load_state_dict(torch.load(filename, map_location=device))\n",
    "        self.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "539ba97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training over 10 epochs: 100%|██████████| 10/10 [00:12<00:00,  1.22s/it, loss=0.001374]\n"
     ]
    }
   ],
   "source": [
    "autoencoder = MNISTAutoencoder(input_dim=2352, latent_dim=64)\n",
    "autoencoder.train_autoencoder(mnist_colors, batch_size=64, epochs=10)\n",
    "latent = autoencoder.encode(mnist_colors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55032e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.5065, -2.1654,  0.8226, -3.2303, -1.2804, -0.3794,  2.8731, -2.7587,\n",
      "         3.7483,  0.4544, -0.3329, -0.9329, -1.4259,  0.7484,  0.6871, -1.7490,\n",
      "        -2.7567,  3.1037, -1.3060,  2.6482,  0.9673, -2.9948, -2.9975, -1.3254,\n",
      "         5.4895,  2.2969,  0.2945,  0.6460, -0.9454, -1.7489, -3.3205,  2.4837,\n",
      "         3.5516,  3.0818,  1.2375,  0.6716, -0.9372, -2.4742, -1.3209, -1.0042,\n",
      "        -3.2583,  1.2475, -0.6093, -0.7634, -0.7961,  0.7088,  1.7656, -0.8446,\n",
      "         1.5638,  1.4276,  5.7076, -4.2542,  4.5978,  1.1984, -3.2465, -1.6063,\n",
      "         5.7736,  2.1653,  3.4059, -0.2373,  1.4847, -1.0380, -2.6935,  1.0564])\n"
     ]
    }
   ],
   "source": [
    "print(latent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
