{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca6b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignallingGame:\n",
    "\n",
    "    def __init__(self, states: int, messages: int, actions: int, seed: int = 42):\n",
    "        self.states = states\n",
    "        self.messages = messages\n",
    "        self.actions = actions\n",
    "        self.message_weights = np.full((states, messages), 1e-6, dtype=float)\n",
    "        self.action_weights = np.full((messages, actions), 1e-6, dtype=float)\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.stats = []\n",
    "\n",
    "\n",
    "    def world_state(self):\n",
    "        return self.rng.randint(self.states)\n",
    "\n",
    "\n",
    "    def emit_message(self, state):\n",
    "        w = self.message_weights[state, :]\n",
    "        probs = w / np.sum(w)\n",
    "        message = self.rng.choice(self.messages, p=probs)\n",
    "        return message\n",
    "\n",
    "    def perform_action(self, message):\n",
    "        w = self.action_weights[message, :]\n",
    "        probs = w / np.sum(w)\n",
    "        action = self.rng.choice(self.actions, p=probs)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def payoff(self, state, action):\n",
    "        return 1 if action == state else 0\n",
    "\n",
    "\n",
    "    def update_weights(self, state, message, action, payoff):\n",
    "        self.message_weights[state, message] += payoff\n",
    "        self.action_weights[message, action] += payoff\n",
    "\n",
    "\n",
    "    def snapshot(self, state, message, action, payoff):\n",
    "        self.stats.append({\n",
    "            \"s\": state,\n",
    "            \"m\": message,\n",
    "            \"a\": action,\n",
    "            \"p\": payoff,\n",
    "            \"mw\": self.message_weights.copy(),\n",
    "            \"aw\": self.action_weights.copy(),\n",
    "        })\n",
    "\n",
    "\n",
    "    def play(self, N: int):\n",
    "        for _ in range(N):\n",
    "            state = self.world_state()\n",
    "            message = self.emit_message(state)\n",
    "            action = self.perform_action(message)\n",
    "            payoff = self.payoff(state, action)\n",
    "            self.update_weights(state, message, action, payoff)\n",
    "            self.snapshot(state, message, action, payoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1277e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcedRothErev(SignallingGame):\n",
    "    def __init__(self, states: int, messages: int, actions: int, l: float, seed: int = 42):\n",
    "        super().__init__(states, messages, actions, seed)\n",
    "\n",
    "        # Convert tabular weights into trainable torch parameters\n",
    "        self.message_weights = torch.nn.Parameter(\n",
    "            torch.tensor(self.message_weights, dtype=torch.float32)\n",
    "        )\n",
    "        self.action_weights = torch.nn.Parameter(\n",
    "            torch.tensor(self.action_weights, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "        # SGD optimizer on both sender and receiver policies\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            [self.message_weights, self.action_weights],\n",
    "            lr=l,\n",
    "        )\n",
    "\n",
    "\n",
    "    def emit_message(self, state):\n",
    "        with torch.no_grad():\n",
    "            logits = self.message_weights[state] # shape: (messages,)\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "        return np.random.choice(self.messages, p=probs)\n",
    "\n",
    "\n",
    "    def perform_action(self, message):\n",
    "        with torch.no_grad():\n",
    "            logits = self.action_weights[message] # shape: (actions,)\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "        return np.random.choice(self.actions, p=probs)\n",
    "\n",
    "\n",
    "    def update_weights(self, state, message, action, payoff):\n",
    "        payoff = torch.tensor(float(payoff), dtype=torch.float32) # scalar\n",
    "\n",
    "        # Sender loss\n",
    "        msg_logits = self.message_weights[state] # (messages,)\n",
    "        log_probs_msg = F.log_softmax(msg_logits, dim=-1)\n",
    "        loss_msg = - payoff * log_probs_msg[message]\n",
    "\n",
    "        # Receiver loss\n",
    "        act_logits = self.action_weights[message] # (actions,)\n",
    "        log_probs_act = F.log_softmax(act_logits, dim=-1)\n",
    "        loss_act = - payoff * log_probs_act[action]\n",
    "\n",
    "        # Combined loss\n",
    "        loss = loss_msg + loss_act\n",
    "\n",
    "        # PyTorch training step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def snapshot(self, state, message, action, payoff):\n",
    "        \"\"\"Snapshot that works with torch parameters.\"\"\"\n",
    "        self.stats.append({\n",
    "            \"s\": state,\n",
    "            \"m\": message,\n",
    "            \"a\": action,\n",
    "            \"p\": payoff,\n",
    "            \"mw\": self.message_weights.detach().cpu().numpy().copy(),\n",
    "            \"aw\": self.action_weights.detach().cpu().numpy().copy(),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0a0cae87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 100\n",
      "First 20 payoffs: [0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "Last 20 payoffs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Total reward: 82\n"
     ]
    }
   ],
   "source": [
    "game = SignallingGame(states=3, messages=3, actions=3)\n",
    "game.play(100)\n",
    "\n",
    "# Payoffs over time\n",
    "payoffs = [x[\"p\"] for x in game.stats]\n",
    "print(\"Number of rounds:\", len(payoffs))\n",
    "print(\"First 20 payoffs:\", payoffs[:20])\n",
    "print(\"Last 20 payoffs:\", payoffs[-20:])\n",
    "print(\"Total reward:\", sum(payoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51405759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 200\n",
      "First 20 payoffs: [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1]\n",
      "Last 20 payoffs: [1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n",
      "Total reward: 111\n"
     ]
    }
   ],
   "source": [
    "game = ReinforcedRothErev(states=3, messages=3, actions=3, l=1)\n",
    "game.play(100)\n",
    "\n",
    "# Payoffs over time\n",
    "payoffs = [x[\"p\"] for x in game.stats]\n",
    "print(\"Number of rounds:\", len(payoffs))\n",
    "print(\"First 20 payoffs:\", payoffs[:20])\n",
    "print(\"Last 20 payoffs:\", payoffs[-20:])\n",
    "print(\"Total reward:\", sum(payoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "992f399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcedRothErev(SignallingGame):\n",
    "    def __init__(self, states: int, messages: int, actions: int, l: float, seed: int = 42):\n",
    "        super().__init__(states, messages, actions, seed)\n",
    "\n",
    "        # Convert tabular weights into trainable torch parameters\n",
    "        self.message_weights = torch.nn.Parameter(\n",
    "            torch.tensor(self.message_weights, dtype=torch.float32)\n",
    "        )\n",
    "        self.action_weights = torch.nn.Parameter(\n",
    "            torch.tensor(self.action_weights, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "        # SGD optimizer on both sender and receiver policies\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            [self.message_weights, self.action_weights],\n",
    "            lr=l,\n",
    "        )\n",
    "\n",
    "\n",
    "    def emit_message(self, state):\n",
    "        with torch.no_grad():\n",
    "            logits = self.message_weights[state] # shape: (messages,)\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "        return np.random.choice(self.messages, p=probs)\n",
    "\n",
    "\n",
    "    def perform_action(self, message):\n",
    "        with torch.no_grad():\n",
    "            logits = self.action_weights[message] # shape: (actions,)\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "        return np.random.choice(self.actions, p=probs)\n",
    "\n",
    "\n",
    "    def update_weights(self, state, message, action, payoff):\n",
    "        payoff = torch.tensor(float(payoff), dtype=torch.float32) # scalar\n",
    "\n",
    "        # Sender loss\n",
    "        msg_logits = self.message_weights[state] # (messages,)\n",
    "        log_probs_msg = F.log_softmax(msg_logits, dim=-1)\n",
    "        loss_msg = - payoff * log_probs_msg[message]\n",
    "\n",
    "        # Receiver loss\n",
    "        act_logits = self.action_weights[message] # (actions,)\n",
    "        log_probs_act = F.log_softmax(act_logits, dim=-1)\n",
    "        loss_act = - payoff * log_probs_act[action]\n",
    "\n",
    "        # Combined loss\n",
    "        loss = loss_msg + loss_act\n",
    "\n",
    "        # PyTorch training step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def snapshot(self, state, message, action, payoff):\n",
    "        \"\"\"Snapshot that works with torch parameters.\"\"\"\n",
    "        self.stats.append({\n",
    "            \"s\": state,\n",
    "            \"m\": message,\n",
    "            \"a\": action,\n",
    "            \"p\": payoff,\n",
    "            \"mw\": self.message_weights.detach().cpu().numpy().copy(),\n",
    "            \"aw\": self.action_weights.detach().cpu().numpy().copy(),\n",
    "        })\n",
    "\n",
    "    def play_batch(self, batch_size):\n",
    "        logprobs_msg = []\n",
    "        logprobs_act = []\n",
    "        rewards = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            state = self.world_state()\n",
    "            message = self.emit_message(state)\n",
    "            action = self.perform_action(message)\n",
    "            payoff = self.payoff(state, action)\n",
    "\n",
    "            # store reward\n",
    "            rewards.append(payoff)\n",
    "\n",
    "            # get log probs for reinforce\n",
    "            msg_logits = self.message_weights[state]\n",
    "            log_probs_msg = F.log_softmax(msg_logits, dim=-1)\n",
    "            logprobs_msg.append(log_probs_msg[message])\n",
    "\n",
    "            act_logits = self.action_weights[message]\n",
    "            log_probs_act = F.log_softmax(act_logits, dim=-1)\n",
    "            logprobs_act.append(log_probs_act[action])\n",
    "\n",
    "            # store stats for user visibility\n",
    "            self.snapshot(state, message, action, payoff)\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        logprobs_msg = torch.stack(logprobs_msg)\n",
    "        logprobs_act = torch.stack(logprobs_act)\n",
    "\n",
    "        # REINFORCE loss for whole batch\n",
    "        loss = - torch.mean(rewards * (logprobs_msg + logprobs_act))\n",
    "\n",
    "        # update parameters\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return float(rewards.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8852d427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total snapshots: 10000\n",
      "First 20 payoffs: [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Last 20 payoffs: [1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1]\n",
      "Total reward: 3893\n"
     ]
    }
   ],
   "source": [
    "game = ReinforcedRothErev(3, 3, 3, l=1)\n",
    "\n",
    "for t in range(100):     # 100 gradient updates\n",
    "    avg_r = game.play_batch(batch_size=100)   # 20 samples per update\n",
    "    # print(f\"Update {t}, average reward = {avg_r:.3f}\")\n",
    "\n",
    "payoffs = [x[\"p\"] for x in game.stats]\n",
    "print(\"Total snapshots:\", len(payoffs))\n",
    "print(\"First 20 payoffs:\", payoffs[:20])\n",
    "print(\"Last 20 payoffs:\", payoffs[-20:])\n",
    "print(\"Total reward:\", sum(payoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b00a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
