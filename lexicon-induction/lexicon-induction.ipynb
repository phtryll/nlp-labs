{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e49c23",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Word alignment with multilingual BERT: from subwords to lexicons\"\n",
    "subtitle: \"Multilingual NLP -- Lab 2\"\n",
    "author: \"Philippos Triantafyllou\"\n",
    "date-modified: last-modified\n",
    "date-format: long\n",
    "lang: en\n",
    "format:\n",
    "    pdf:\n",
    "        pdf-engine: lualatex\n",
    "        documentclass: scrartcl\n",
    "        fontsize: 16pt\n",
    "        papersize: A3\n",
    "        toccolor: blue\n",
    "        classoption: \n",
    "            - \"DIV=12\"\n",
    "            - \"parskip=relative\"\n",
    "            - \"titlepage=false\"\n",
    "        code-block-border-left: MediumBlue\n",
    "        code-block-bg: WhiteSmoke\n",
    "        template-partials:\n",
    "            - \"../_pandoc/doc-class.tex\"\n",
    "            - \"../_pandoc/toc.tex\"\n",
    "            - \"../_pandoc/before-title.tex\"\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "number-depth: 1\n",
    "number-sections: true\n",
    "highlight-style: github\n",
    "fig-cap-location: top\n",
    "execute:\n",
    "  echo: false\n",
    "  output: false\n",
    "embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507e202",
   "metadata": {},
   "source": [
    "## Selecting languages\n",
    "\n",
    "We will load data form `sentence-transformers/parallel-sentences-jw300` and bilingual pairs from `MUSE`.\n",
    "\n",
    "We select 5 languages that are well represented in the training of `mBERT` and 5 others that are either less represented or absent (the only language that is completely absent from `mBERT` is Kurdish). All the languages also have a bilingual lexicon from `MUSE` that we will use for the CCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2c4b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable useless logging\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from datasets import disable_progress_bars\n",
    "disable_progress_bars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85593d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "# (language, code, link_to_MUSE)\n",
    "langs = [\n",
    "    (\"albanian\",    \"sq\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-sq.0-5000.txt\"),\n",
    "    (\"arabic\",      \"ar\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-ar.0-5000.txt\"),\n",
    "    (\"french\",      \"fr\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-fr.0-5000.txt\"),\n",
    "    (\"german\",      \"de\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-de.0-5000.txt\"),\n",
    "    (\"greek\",       \"el\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-el.0-5000.txt\"),\n",
    "    (\"hindi\",       \"hi\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.0-5000.txt\"),\n",
    "    (\"japanese\",    \"ja\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-ja.0-5000.txt\"),\n",
    "    (\"macedonian\",  \"mk\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-mk.0-5000.txt\"),\n",
    "    (\"persian\",     \"fa\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-fa.0-5000.txt\"),\n",
    "    (\"thai\",        \"th\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-th.0-5000.txt\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca18a2",
   "metadata": {},
   "source": [
    "Loading the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe8c7260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "\n",
    "def install_datasets(langs):\n",
    "    corpora = {}\n",
    "\n",
    "    # Iterate through each language definition (name, ISO code, lexicon URL)\n",
    "    for lang, code, links in langs:\n",
    "        print(f\"Processing {lang}...\")\n",
    "        corpora[lang] = {}\n",
    "        print(f\"\\tLoading pairs...\")\n",
    "\n",
    "        # Load parallel English–target-language sentence pairs from JW300\n",
    "        data = load_dataset(\n",
    "            \"sentence-transformers/parallel-sentences-jw300\",\n",
    "            f\"en-{code}\",\n",
    "            split=\"train\"\n",
    "        )\n",
    "        print(f\"\\tLoaded JW300 for {lang}\")\n",
    "\n",
    "        corpora[lang]['pairs'] = (\n",
    "            data\n",
    "            # Load up to 5,000 parallel sentence pairs\n",
    "            .take(5000) # type: ignore\n",
    "            .map(\n",
    "                # Clean zero-width Unicode chars\n",
    "                lambda x: {\n",
    "                    'english': x['english'].replace('\\u200b', '').replace('\\u200f', '').replace('\\u200c', ''),\n",
    "                    'non_english': x['non_english'].replace('\\u200b', '').replace('\\u200f', '').replace('\\u200c', '')\n",
    "                }\n",
    "            )\n",
    "            # Drop any pair where either side becomes empty after cleaning\n",
    "            .filter(\n",
    "                lambda x: x['english'].strip() and x['non_english'].strip()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Report how many valid parallel pairs remain after filtering\n",
    "        print(f\"\\tKept {len(corpora[lang]['pairs'])} aligned sentence pairs\")\n",
    "\n",
    "        print(f\"\\tLoading lexicons...\")\n",
    "\n",
    "        # Download and decode the lexicon text file\n",
    "        text = requests.get(links).content.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        # Parse tab-separated lexicons if tabs are present\n",
    "        if '\\t' in text:\n",
    "            lexicon = [\n",
    "                tuple(line.split('\\t', 1))\n",
    "                for line in text.splitlines()\n",
    "                if line.strip() and '\\t' in line\n",
    "            ]\n",
    "        # Otherwise parse whitespace-separated lexicons (French and German)\n",
    "        else:\n",
    "            lexicon = [\n",
    "                tuple(line.split(None, 1))\n",
    "                for line in text.splitlines()\n",
    "                if line.strip()\n",
    "            ]\n",
    "\n",
    "        corpora[lang][\"lexicon\"] = lexicon\n",
    "    return corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eda3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "def save_datasets(corpora, data_folder=\"data\"):\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    \n",
    "    for lang, content in corpora.items():\n",
    "        lang_folder = os.path.join(data_folder, lang)\n",
    "        os.makedirs(lang_folder, exist_ok=True)\n",
    "        \n",
    "        # Save pairs as jsonl\n",
    "        pairs_path = os.path.join(lang_folder, \"pairs.jsonl\")\n",
    "        content['pairs'].to_json(pairs_path)\n",
    "        \n",
    "        # Save lexicon as text file\n",
    "        lexicon_path = os.path.join(lang_folder, \"lexicon.txt\")\n",
    "        with open(lexicon_path, 'w', encoding='utf-8') as f:\n",
    "            for src, tgt in content['lexicon']:\n",
    "                f.write(f\"{src}\\t{tgt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0552a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(langs, data_folder=\"data\"):\n",
    "    corpora = {}\n",
    "    \n",
    "    for lang, _, _ in langs:\n",
    "        lang_folder = os.path.join(data_folder, lang)\n",
    "        corpora[lang] = {}\n",
    "        \n",
    "        # Load pairs\n",
    "        pairs_path = os.path.join(lang_folder, \"pairs.jsonl\")\n",
    "        corpora[lang]['pairs'] = Dataset.from_json(pairs_path)\n",
    "        \n",
    "        # Load lexicon\n",
    "        lexicon_path = os.path.join(lang_folder, \"lexicon.txt\")\n",
    "        with open(lexicon_path, 'r', encoding='utf-8') as f:\n",
    "            lexicon = [tuple(line.strip().split('\\t', 1)) for line in f if line.strip()]\n",
    "        corpora[lang]['lexicon'] = lexicon\n",
    "    \n",
    "    return corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b2d30a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing albanian...\n",
      "\tLoading pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c1be7aae3c444c9949bb0e51c28572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0354a859c70845d38fbe7ec77a5edebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-sq/train-00000-of-00001.parquet:   0%|          | 0.00/83.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoaded JW300 for albanian\n",
      "\tKept 4928 aligned sentence pairs\n",
      "\tLoading lexicons...\n",
      "Processing arabic...\n",
      "\tLoading pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11b9ce8043c4e2786931c8fb1e8e33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-ar/train-00000-of-00001.parquet:   0%|          | 0.00/71.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoaded JW300 for arabic\n",
      "\tKept 4844 aligned sentence pairs\n",
      "\tLoading lexicons...\n",
      "Processing french...\n",
      "\tLoading pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4c07e85b864297a2e3f64aae43df7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-fr/train-00000-of-00001.parquet:   0%|          | 0.00/297M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoaded JW300 for french\n",
      "\tKept 4773 aligned sentence pairs\n",
      "\tLoading lexicons...\n",
      "Processing german...\n",
      "\tLoading pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdc9183dd9544f6b726b43e660ef099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-de/train-00000-of-00001.parquet:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoaded JW300 for german\n",
      "\tKept 4942 aligned sentence pairs\n",
      "\tLoading lexicons...\n",
      "Processing greek...\n",
      "\tLoading pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c7717ad08647ada0c1cd612032dab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-el/train-00000-of-00002.parquet:   0%|          | 0.00/173M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60ab6b982d24ea19d24922082399065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-el/train-00001-of-00002.parquet:   0%|          | 0.00/174M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoaded JW300 for greek\n",
      "\tKept 4618 aligned sentence pairs\n",
      "\tLoading lexicons...\n",
      "Processing hindi...\n",
      "\tLoading pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d711a1839d4ed0b9134141933d8632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-hi/train-00000-of-00001.parquet:   0%|          | 0.00/86.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoaded JW300 for hindi\n",
      "\tKept 4053 aligned sentence pairs\n",
      "\tLoading lexicons...\n",
      "Processing japanese...\n",
      "\tLoading pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2538893ff846e2ae5b557282742e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-ja/train-00000-of-00002.parquet:   0%|          | 0.00/142M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f1558fd04a4709ae47307a290752d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-ja/train-00001-of-00002.parquet:   0%|          | 0.00/136M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoaded JW300 for japanese\n",
      "\tKept 4557 aligned sentence pairs\n",
      "\tLoading lexicons...\n",
      "Processing macedonian...\n",
      "\tLoading pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399b23ba840f4d47a851f1504e9dc9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-mk/train-00000-of-00001.parquet:   0%|          | 0.00/66.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoaded JW300 for macedonian\n",
      "\tKept 4890 aligned sentence pairs\n",
      "\tLoading lexicons...\n",
      "Processing persian...\n",
      "\tLoading pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c425d7129b5e41ff863af10b01610088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-fa/train-00000-of-00001.parquet:   0%|          | 0.00/40.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoaded JW300 for persian\n",
      "\tKept 4511 aligned sentence pairs\n",
      "\tLoading lexicons...\n",
      "Processing thai...\n",
      "\tLoading pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bd045f54d34eb8b865aec1ad12563b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-th/train-00000-of-00001.parquet:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoaded JW300 for thai\n",
      "\tKept 4167 aligned sentence pairs\n",
      "\tLoading lexicons...\n"
     ]
    }
   ],
   "source": [
    "#| output: true\n",
    "#| eval: false\n",
    "\n",
    "data = install_datasets(langs)\n",
    "save_datasets(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f18f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_datasets(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f31d9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For albanian:\n",
      "pairs (<class 'datasets.arrow_dataset.Dataset'>)\n",
      "{'english': ['Page Two', '1945 - 1995 What Have We Learned?', '3 - 14', 'Fifty years have passed since the end of World War II.', 'In what ways has humankind progressed?'], 'non_english': ['Faqja dy', '1945 - 1995 Çfarë kemi mësuar?', '3 - 14', 'Kanë kaluar pesëdhjetë vjet që nga mbarimi i Luftës II Botërore.', 'Në cilat fusha ka bërë progres njeriu?']}\n",
      "lexicon (<class 'list'>)\n",
      "[('and', 'dhe'), ('was', 'ishte'), ('for', 'për'), ('for', 'per'), ('that', 'që')]\n",
      "\n",
      "For arabic:\n",
      "pairs (<class 'datasets.arrow_dataset.Dataset'>)\n",
      "{'english': ['Perhaps you have asked yourself this question, as well as others: “How Can I Say No to Premarital Sex? ”', '“ Why Do I Get So Depressed? ”', 'These are chapter titles in the new book Questions Young People Ask  — Answers That Work.', 'A youth from San Antonio, Texas, wrote: “I really enjoyed chapter 36, ‘ How Can I Control My TV Viewing Habits? ’', 'It will really help me. I am a compulsive TV watcher.'], 'non_english': ['كيف يمكنني ان اضبط عادات مشاهدتي التلفزيون ؟', 'ربما طرحتم على أنفسكم هذا السؤال ، بالإضافة الى اسئلة اخرى مثل : « كيف يمكنني ان اقول لا للجنس قبل الزواج ؟ » ، « كيف اعرف ما اذا كان حبا حقيقيا ؟ » ، و « لماذا أكتئب الى هذا الحد ؟ » .', 'ان هذه الاسئلة هي عناوين بعض الفصول من كتاب اسئلة يطرحها الاحداث — اجوبة تنجح .', 'كتب حدث من سان أنطونيو : « لقد تمتعتُ جدا بقراءة الفصل ٣٦ بعنوان : ‹ كيف يمكنني ان اضبط عادات مشاهدتي التلفزيون ؟ › .', 'سيكون هذا الفصل خير مساعد لي .']}\n",
      "lexicon (<class 'list'>)\n",
      "[('the', 'the'), ('the', 'ال'), ('and', 'وال'), ('was', 'كانت'), ('was', 'كان')]\n",
      "\n",
      "For french:\n",
      "pairs (<class 'datasets.arrow_dataset.Dataset'>)\n",
      "{'english': ['“ A Good Word for the Witnesses ”', 'THE preaching activity of Jehovah’s witnesses is growing very rapidly.', 'This has required a large expansion of facilities at their international headquarters in Brooklyn, New York.', 'The expansion is arousing much comment in the community, even prompting a sermon at the Plymouth Church (Congregational), located just two blocks away.', 'More than a century ago, the church’s first minister, Henry Ward Beecher, lived on property that is now part of the Watchtower Society’s headquarters complex.'], 'non_english': ['“ Éloge des Témoins ”', 'L’ŒUVRE de prédication des témoins de Jéhovah s’étend rapidement.', 'Cette extension a exigé l’agrandissement de leur siège principal situé à Brooklyn, New York.', 'Pareille expansion suscite de nombreux commentaires dans la localité et a même été l’objet d’un prêche prononcé dans le temple Plymouth (de l’Église congrégationaliste), situé à deux pâtés de maisons du siège des témoins de Jéhovah.', 'Il y a plus d’un siècle, le premier pasteur de ce temple, Henry Ward Beecher, habitait une maison qui fait partie aujourd’hui de l’ensemble des bâtiments appartenant à la Société Watchtower.']}\n",
      "lexicon (<class 'list'>)\n",
      "[('the', 'le'), ('the', 'les'), ('the', 'la'), ('and', 'et'), ('was', 'fut')]\n",
      "\n",
      "For german:\n",
      "pairs (<class 'datasets.arrow_dataset.Dataset'>)\n",
      "{'english': ['“ A Good Word for the Witnesses ”', 'THE preaching activity of Jehovah’s witnesses is growing very rapidly.', 'This has required a large expansion of facilities at their international headquarters in Brooklyn, New York.', 'The expansion is arousing much comment in the community, even prompting a sermon at the Plymouth Church (Congregational), located just two blocks away.', 'More than a century ago, the church’s first minister, Henry Ward Beecher, lived on property that is now part of the Watchtower Society’s headquarters complex.'], 'non_english': ['„ Ein Wort zugunsten der Zeugen Jehovas “', 'DAS Predigtwerk der Zeugen Jehovas dehnt sich sehr schnell aus.', 'Deshalb mußte ihr Hauptsitz in Brooklyn, New York (USA), vergrößert werden.', 'Diese Vergrößerung bildet einen Gesprächsstoff in Brooklyn; in der Plymouth - Kirche (Kongregationalisten), zwei Häuserblocks vom Hauptsitz entfernt, wurde sogar eine Predigt über dieses Thema gehalten.', 'Vor mehr als hundert Jahren wohnte der erste Pfarrer dieser Kirche, Henry Ward Beecher, in einem Haus, das auf Boden gebaut war, der jetzt zu dem Grund gehört, auf dem die Gebäude des Hauptsitzes der Watchtower Society stehen.']}\n",
      "lexicon (<class 'list'>)\n",
      "[('the', 'die'), ('the', 'der'), ('the', 'dem'), ('the', 'den'), ('the', 'das')]\n",
      "\n",
      "For greek:\n",
      "pairs (<class 'datasets.arrow_dataset.Dataset'>)\n",
      "{'english': ['“ A Good Word for the Witnesses ”', 'THE preaching activity of Jehovah’s witnesses is growing very rapidly.', 'This has required a large expansion of facilities at their international headquarters in Brooklyn, New York.', 'The expansion is arousing much comment in the community, even prompting a sermon at the Plymouth Church (Congregational), located just two blocks away.', 'More than a century ago, the church’s first minister, Henry Ward Beecher, lived on property that is now part of the Watchtower Society’s headquarters complex.'], 'non_english': ['« Ένας Καλός Λόγος για τους Μάρτυρας »', 'Η ΔΡΑΣΙΣ κηρύγματος των μαρτύρων του Ιεχωβά μεγαλώνει πολύ γρήγορα.', 'Αυτό χρειάσθηκε μια μεγάλη επέκτασι ευκολιών στα διεθνή των γραφεία στο Μπρούκλυν της Νέας Υόρκης.', 'Η επέκτασις εγείρει πολλά σχόλια στην κοινότητα, και προκάλεσε ακόμη την εκφώνησι μιας ομιλίας στην Κογκρεγκασιοναλιστική Εκκλησία του Πλύμουθ, που κείται δύο μόνον τετράγωνα μακρυά.', 'Πριν από εκατό χρόνια και πλέον, ο πρώτος λειτουργός της εκκλησίας αυτής, ο Χένρυ Ουώρντ Μπήτσερ, διέμενε σε ιδιοκτησία που τώρα αποτελεί μέρος του συμπλέγματος των γραφείων της Εταιρίας Σκοπιά.']}\n",
      "lexicon (<class 'list'>)\n",
      "[('the', 'το'), ('and', 'και'), ('was', 'ήταν'), ('for', 'για'), ('for', 'for')]\n",
      "\n",
      "For hindi:\n",
      "pairs (<class 'datasets.arrow_dataset.Dataset'>)\n",
      "{'english': ['Big Business and Crime', 'BIG BUSINESS!', 'It affects all of us. It helps us  — and it harms us.', 'And there are things we can do about it. A giant, or “big, ” corporation may have assets worth $1,500,000,000.', 'Many have far more. That kind of money represents power.'], 'non_english': ['बड़ा व्यवसाय और अपराध', 'बड़ा व्यवसाय!', 'हम सब पर इसका प्रभाव पड़ता है ।', 'यह हमारी सहायता करता है — और हमें हानि भी पहुँचाता है ।', 'और कुछ ऐसी बातें हैं जो हम उसके बारे में कर सकते हैं ।']}\n",
      "lexicon (<class 'list'>)\n",
      "[('and', 'और'), ('was', 'था'), ('was', 'थी'), ('for', 'लिये'), ('that', 'उस')]\n",
      "\n",
      "For japanese:\n",
      "pairs (<class 'datasets.arrow_dataset.Dataset'>)\n",
      "{'english': ['“ A Good Word for the Witnesses ”', 'THE preaching activity of Jehovah’s witnesses is growing very rapidly.', 'This has required a large expansion of facilities at their international headquarters in Brooklyn, New York.', 'The expansion is arousing much comment in the community, even prompting a sermon at the Plymouth Church (Congregational), located just two blocks away.', 'More than a century ago, the church’s first minister, Henry Ward Beecher, lived on property that is now part of the Watchtower Society’s headquarters complex.'], 'non_english': ['「エホバ  の  証人  へ  の  賛辞」', 'エホバ  の  証人  の  伝道  活動  は  急速  に  発展  し  た  ため ， ニューヨーク  市  ブルックリン  に  ある  その  国際  本部  の  施設  は  大いに  拡張  さ  れ  まし  た。', 'この  こと  は  その  付近  の  人々  の  話題  に  も  のぼり ， 本部  から  2  区画  先  の  プリマス  教会 （ 組合  教会 ） では ， 説教  の  中  に  さえ  取り上げ  られ  まし  た。', 'さて ， 「エホバ  の  証人  へ  の  賛辞」と  題する  その  説教  を  し  た  ハリー  ･  H  ･  クルーナー  博士  は ， まず  最初  に  こう  述べ  まし  た。「', '真  の  宗教  に  とっ  て  肝要  な  幾多  の  教理  の  面  で ， エホバ  の  証人  は  まったく  誤っ  て  いる ， と  言わ  ざる  を  得  ませ  ん」。']}\n",
      "lexicon (<class 'list'>)\n",
      "[('and', 'そして'), ('was', 'was'), ('for', 'for'), ('from', 'から'), ('this', 'この')]\n",
      "\n",
      "For macedonian:\n",
      "pairs (<class 'datasets.arrow_dataset.Dataset'>)\n",
      "{'english': ['Young People Ask...', 'How Can I Cope With Injustice?', '“ Only those who have money are respected, but we who don’t even have anything to eat or anywhere to sleep are treated like animals.', 'If I expect anything for the future, it is to die without anyone taking notice. ” — Arnulfo, a 15 - year - old homeless boy.', 'THERE is much injustice in the world.'], 'non_english': ['Младите прашуваат...', 'Како можам да се справам со неправдата?', '„ Ги почитуваат само оние што имаат пари, додека нас кои немаме дури ни што да јадеме ниту каде да спиеме, нѐ третираат како животни.', 'Ако очекувам нешто во иднината, тоа е да умрам и никој да не го забележи тоа “(Арнулфо, 15 - годишно бездомно момче).', 'ВО СВЕТОТ има многу неправди.']}\n",
      "lexicon (<class 'list'>)\n",
      "[('the', 'на'), ('was', 'била'), ('was', 'беше'), ('for', 'за'), ('that', 'тоа')]\n",
      "\n",
      "For persian:\n",
      "pairs (<class 'datasets.arrow_dataset.Dataset'>)\n",
      "{'english': ['© 2016 Watch Tower Bible and Tract Society of Pennsylvania', 'This publication is not for sale. It is provided as part of a worldwide Bible educational work supported by voluntary donations.', 'To make a donation, please visit www.jw.org.', 'Unless otherwise indicated, Scripture quotations are from the modern - language New World Translation of the Holy Scriptures.', 'Table of Contents'], 'non_english': ['© 2016 International Bible Students Association', 'این نشریه برای فروش نیست و جهت آموزش جهانی کتاب مقدّس تهیه شده است و مخارج آن از طریق اعانات تأمین میگردد .', 'برای اهدای اعانه میتوانید به وبسایت www.jw.org / fa مراجعه نمایید .', 'در غیر این صورت نام ترجمهٔ مورد نظر ذکر شده است .', 'فهرست']}\n",
      "lexicon (<class 'list'>)\n",
      "[('the', 'را'), ('was', 'بود'), ('for', 'برای'), ('that', 'که'), ('with', 'با')]\n",
      "\n",
      "For thai:\n",
      "pairs (<class 'datasets.arrow_dataset.Dataset'>)\n",
      "{'english': ['Religion’s Future in View of Its Past', 'Part 13  — 476 C.E. onward  — Out of Darkness, Something “Holy ”', '“ Sins committed in the dark are seen in Heaven like sheets of fire. ”  — Chinese proverb', 'IN APRIL 1988 the Church in the Soviet Union rejoiced to hear General Secretary Mikhail Gorbachev publicly state that mistakes made by the State in its relationship with the Church and its members were to be corrected.', 'A rift of another kind also seemed to be on its way to settlement when Roman Catholic pope John Paul II sent greetings to the “thousand - year - old sister church as an expression of the heartfelt desire to achieve that perfect union that Christ wanted and that is basic to the nature of the Church. ”'], 'non_english': ['ตอน  ที่ 13: จาก ส.', 'ศ. 476 เป็น  ต้น  มา — สิ่ง  ที่  ถือ  ว่า “ศักดิ์สิทธิ์ ” จาก  ความ  มืด', '“ บาป  ซึ่ง  ได้  กระทำ  ใน  ที่  มืด  ปรากฏ  ดุจ  เปลว  เพลิง  ใน  สวรรค์. ” — สุภาษิต  จีน', 'เดือน  เมษายน 1988 คริสต์  จักร  ใน  สหภาพ  โซเวียต  พา  กัน  ชื่นชม  ที่  ได้  ยิน  เลขาธิการ  ใหญ่  มิ คา อิล กอร์ บา ชอฟ  แถลง  อย่าง  เปิด  เผย  ว่า  ข้อ  ผิด  พลาด  ต่าง ๆ ใน  ความ  สัมพันธ์  ที่  รัฐ  ได้  กระทำ  ต่อ  คริสต์  ศาสนจักร  และ  คริสต์  สมาชิก  จะ  ต้อง  ได้  รับ  การ  แก้ไข.', 'แต่  รอย  แตก  ระหว่าง ‘ คริสต์  จักร  พี่  กับ  น้อง ’ เดิม  ที  เกิด  ขึ้น  มา  อย่าง  ไร?']}\n",
      "lexicon (<class 'list'>)\n",
      "[('the', 'เดอะ'), ('and', 'และ'), ('and', 'แล้ว'), ('for', 'สำหรับ'), ('for', 'เพื่อ')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lang, corpora in data.items():\n",
    "    print(f\"For {lang}:\")\n",
    "    for keys, vals in corpora.items():\n",
    "        print(f\"{keys} ({type(vals)})\")\n",
    "        print(f\"{vals[:5]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9881a6",
   "metadata": {},
   "source": [
    "## Obtaining embeddings with `mBERT`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d991f2",
   "metadata": {},
   "source": [
    "For the selected languages, obtain the `mBERT` representations for all these sentences, aggregating subword embeddings in order to reconstruct word-level vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46f087b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617f074c81224f57844a1fed649af253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9b5aa1ffa14f239ea55e22c322f01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3caada71654dcc869cb326ccbc7b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6db9664f2044be867569b7c25436a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfcf4b63eeb4c54b92c57721516f0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: true\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3f10c3",
   "metadata": {},
   "source": [
    "The pipeline is as follows:\n",
    "\n",
    "- A function takes care to extract the embeddings for a list of sentences in a given language.\n",
    "- A second function iterates over all languages in the dataset, calling the first function for each language and storing the results.\n",
    "- Finally, we save the extracted embeddings to disk for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33488bbc",
   "metadata": {},
   "source": [
    "Function to extract embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_word_embeddings(sentences, tokenizer, model, batch_size=16, device=None):\n",
    "    # Device detection\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_results = []\n",
    "    total_sentences = len(sentences)\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_idx in range(0, total_sentences, batch_size):\n",
    "        batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "        # Tokenize batch\n",
    "        encoded = tokenizer(\n",
    "            list(batch),\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Move to device\n",
    "        input_ids = encoded['input_ids'].to(device)\n",
    "        attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = outputs.last_hidden_state # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # Process each sentence in the batch\n",
    "        for idx_in_batch, sentence in enumerate(batch):\n",
    "            word_ids = encoded.word_ids(batch_index=idx_in_batch)\n",
    "            sent_embeddings = embeddings[idx_in_batch]\n",
    "\n",
    "            # Group tokens by word_id\n",
    "            word_to_token_indices = {}\n",
    "            for token_idx, word_id in enumerate(word_ids):\n",
    "                if word_id is not None:\n",
    "                    if word_id not in word_to_token_indices:\n",
    "                        word_to_token_indices[word_id] = []\n",
    "                    word_to_token_indices[word_id].append(token_idx)\n",
    "\n",
    "            # Aggregate embeddings (mean)\n",
    "            word_embeddings = []\n",
    "            for word_id in sorted(word_to_token_indices.keys()):\n",
    "                token_indices = word_to_token_indices[word_id]\n",
    "                token_embs = sent_embeddings[token_indices]\n",
    "                word_emb = token_embs.mean(dim=0)\n",
    "\n",
    "                word_embeddings.append({\n",
    "                    'word_id': word_id,\n",
    "                    'embedding': word_emb.cpu().numpy(),\n",
    "                    'num_tokens': len(token_indices)\n",
    "                })\n",
    "\n",
    "            all_results.append({\n",
    "                'sentence': sentence,\n",
    "                'word_embeddings': word_embeddings,\n",
    "                'num_words': len(word_embeddings)\n",
    "            })\n",
    "\n",
    "        # Progress\n",
    "        processed = min(batch_idx + batch_size, total_sentences)\n",
    "        progress_pct = (processed / total_sentences) * 100\n",
    "        print(f\"\\tProgress: {processed}/{total_sentences} ({progress_pct:.1f}%)\", end='\\r')\n",
    "\n",
    "        # Memory management\n",
    "        if device.type == 'cpu' and batch_idx % (batch_size * 20) == 0:\n",
    "            import gc\n",
    "            gc.collect()\n",
    "\n",
    "        if device.type == 'cuda' and batch_idx % (batch_size * 10) == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print()\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c3b99",
   "metadata": {},
   "source": [
    "Function to process all languages in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4644d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "def process_all_languages(data, tokenizer, model, batch_size=16, device=None, save_dir=None):\n",
    "    import time\n",
    "    \n",
    "    embeddings_by_language = {}\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for lang_idx, (lang, corpora) in enumerate(data.items(), 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{lang_idx}/{len(data)}] Processing {lang.upper()}\")\n",
    "        print('='*70)\n",
    "        \n",
    "        embeddings_by_language[lang] = {}\n",
    "        \n",
    "        if 'pairs' in corpora:\n",
    "            english_sentences = corpora['pairs']['english']\n",
    "            non_english_sentences = corpora['pairs']['non_english']\n",
    "            \n",
    "            # Process English\n",
    "            print(f\"\\nProcessing English sentences ({len(english_sentences)} total)...\")\n",
    "            start_time = time.time()\n",
    "            embeddings_by_language[lang]['english'] = get_word_embeddings(\n",
    "                english_sentences, tokenizer, model, \n",
    "                batch_size=batch_size, device=device\n",
    "            )\n",
    "            english_time = time.time() - start_time\n",
    "            print(f\"\\tCompleted in {english_time:.2f} seconds\")\n",
    "            \n",
    "            # Process non-English\n",
    "            print(f\"\\nProcessing {lang.capitalize()} sentences ({len(non_english_sentences)} total)...\")\n",
    "            start_time = time.time()\n",
    "            embeddings_by_language[lang]['non_english'] = get_word_embeddings(\n",
    "                non_english_sentences, tokenizer, model, \n",
    "                batch_size=batch_size, device=device\n",
    "            )\n",
    "            non_english_time = time.time() - start_time\n",
    "            print(f\"\\tCompleted in {non_english_time:.2f} seconds\")\n",
    "            \n",
    "            # Summary\n",
    "            print(f\"\\n{lang.upper()} Summary:\")\n",
    "            print(f\"\\tEnglish: {len(embeddings_by_language[lang]['english'])} sentences\")\n",
    "            print(f\"\\t{lang.capitalize()}: {len(embeddings_by_language[lang]['non_english'])} sentences\")\n",
    "            print(f\"\\tTotal time: {english_time + non_english_time:.2f} seconds\")\n",
    "            \n",
    "            # Save intermediate results\n",
    "            if save_dir:\n",
    "                import pickle\n",
    "                import os\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                \n",
    "                filepath = os.path.join(save_dir, f'embeddings_{lang}.pkl')\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    pickle.dump(embeddings_by_language[lang], f)\n",
    "                print(f\"\\tSaved to {filepath}\")\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ALL LANGUAGES PROCESSED!\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    print('='*70)\n",
    "    \n",
    "    return embeddings_by_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db083de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[1/10] Processing ALBANIAN\n",
      "======================================================================\n",
      "\n",
      "Processing English sentences (4928 total)...\n",
      "\tProgress: 4928/4928 (100.0%)\n",
      "\tCompleted in 15.18 seconds\n",
      "\n",
      "Processing Albanian sentences (4928 total)...\n",
      "\tProgress: 4928/4928 (100.0%)\n",
      "\tCompleted in 16.46 seconds\n",
      "\n",
      "ALBANIAN Summary:\n",
      "\tEnglish: 4928 sentences\n",
      "\tAlbanian: 4928 sentences\n",
      "\tTotal time: 31.63 seconds\n",
      "\tSaved to ./embeddings/embeddings_albanian.pkl\n",
      "\n",
      "======================================================================\n",
      "[2/10] Processing ARABIC\n",
      "======================================================================\n",
      "\n",
      "Processing English sentences (4844 total)...\n",
      "\tProgress: 4844/4844 (100.0%)\n",
      "\tCompleted in 13.99 seconds\n",
      "\n",
      "Processing Arabic sentences (4844 total)...\n",
      "\tProgress: 4844/4844 (100.0%)\n",
      "\tCompleted in 12.94 seconds\n",
      "\n",
      "ARABIC Summary:\n",
      "\tEnglish: 4844 sentences\n",
      "\tArabic: 4844 sentences\n",
      "\tTotal time: 26.93 seconds\n",
      "\tSaved to ./embeddings/embeddings_arabic.pkl\n",
      "\n",
      "======================================================================\n",
      "[3/10] Processing FRENCH\n",
      "======================================================================\n",
      "\n",
      "Processing English sentences (4773 total)...\n",
      "\tProgress: 4773/4773 (100.0%)\n",
      "\tCompleted in 14.68 seconds\n",
      "\n",
      "Processing French sentences (4773 total)...\n",
      "\tProgress: 4773/4773 (100.0%)\n",
      "\tCompleted in 16.95 seconds\n",
      "\n",
      "FRENCH Summary:\n",
      "\tEnglish: 4773 sentences\n",
      "\tFrench: 4773 sentences\n",
      "\tTotal time: 31.63 seconds\n",
      "\tSaved to ./embeddings/embeddings_french.pkl\n",
      "\n",
      "======================================================================\n",
      "[4/10] Processing GERMAN\n",
      "======================================================================\n",
      "\n",
      "Processing English sentences (4942 total)...\n",
      "\tProgress: 4942/4942 (100.0%)\n",
      "\tCompleted in 14.40 seconds\n",
      "\n",
      "Processing German sentences (4942 total)...\n",
      "\tProgress: 4942/4942 (100.0%)\n",
      "\tCompleted in 14.99 seconds\n",
      "\n",
      "GERMAN Summary:\n",
      "\tEnglish: 4942 sentences\n",
      "\tGerman: 4942 sentences\n",
      "\tTotal time: 29.38 seconds\n",
      "\tSaved to ./embeddings/embeddings_german.pkl\n",
      "\n",
      "======================================================================\n",
      "[5/10] Processing GREEK\n",
      "======================================================================\n",
      "\n",
      "Processing English sentences (4618 total)...\n",
      "\tProgress: 4618/4618 (100.0%)\n",
      "\tCompleted in 15.51 seconds\n",
      "\n",
      "Processing Greek sentences (4618 total)...\n",
      "\tProgress: 4618/4618 (100.0%)\n",
      "\tCompleted in 16.49 seconds\n",
      "\n",
      "GREEK Summary:\n",
      "\tEnglish: 4618 sentences\n",
      "\tGreek: 4618 sentences\n",
      "\tTotal time: 32.00 seconds\n",
      "\tSaved to ./embeddings/embeddings_greek.pkl\n",
      "\n",
      "======================================================================\n",
      "[6/10] Processing HINDI\n",
      "======================================================================\n",
      "\n",
      "Processing English sentences (4053 total)...\n",
      "\tProgress: 4053/4053 (100.0%)\n",
      "\tCompleted in 12.53 seconds\n",
      "\n",
      "Processing Hindi sentences (4053 total)...\n",
      "\tProgress: 4053/4053 (100.0%)\n",
      "\tCompleted in 11.72 seconds\n",
      "\n",
      "HINDI Summary:\n",
      "\tEnglish: 4053 sentences\n",
      "\tHindi: 4053 sentences\n",
      "\tTotal time: 24.25 seconds\n",
      "\tSaved to ./embeddings/embeddings_hindi.pkl\n",
      "\n",
      "======================================================================\n",
      "[7/10] Processing JAPANESE\n",
      "======================================================================\n",
      "\n",
      "Processing English sentences (4557 total)...\n",
      "\tProgress: 4557/4557 (100.0%)\n",
      "\tCompleted in 13.95 seconds\n",
      "\n",
      "Processing Japanese sentences (4557 total)...\n",
      "\tProgress: 4557/4557 (100.0%)\n",
      "\tCompleted in 19.01 seconds\n",
      "\n",
      "JAPANESE Summary:\n",
      "\tEnglish: 4557 sentences\n",
      "\tJapanese: 4557 sentences\n",
      "\tTotal time: 32.96 seconds\n",
      "\tSaved to ./embeddings/embeddings_japanese.pkl\n",
      "\n",
      "======================================================================\n",
      "[8/10] Processing MACEDONIAN\n",
      "======================================================================\n",
      "\n",
      "Processing English sentences (4890 total)...\n",
      "\tProgress: 4890/4890 (100.0%)\n",
      "\tCompleted in 13.57 seconds\n",
      "\n",
      "Processing Macedonian sentences (4890 total)...\n",
      "\tProgress: 4890/4890 (100.0%)\n",
      "\tCompleted in 14.11 seconds\n",
      "\n",
      "MACEDONIAN Summary:\n",
      "\tEnglish: 4890 sentences\n",
      "\tMacedonian: 4890 sentences\n",
      "\tTotal time: 27.69 seconds\n",
      "\tSaved to ./embeddings/embeddings_macedonian.pkl\n",
      "\n",
      "======================================================================\n",
      "[9/10] Processing PERSIAN\n",
      "======================================================================\n",
      "\n",
      "Processing English sentences (4511 total)...\n",
      "\tProgress: 4511/4511 (100.0%)\n",
      "\tCompleted in 14.39 seconds\n",
      "\n",
      "Processing Persian sentences (4511 total)...\n",
      "\tProgress: 4511/4511 (100.0%)\n",
      "\tCompleted in 13.86 seconds\n",
      "\n",
      "PERSIAN Summary:\n",
      "\tEnglish: 4511 sentences\n",
      "\tPersian: 4511 sentences\n",
      "\tTotal time: 28.25 seconds\n",
      "\tSaved to ./embeddings/embeddings_persian.pkl\n",
      "\n",
      "======================================================================\n",
      "[10/10] Processing THAI\n",
      "======================================================================\n",
      "\n",
      "Processing English sentences (4167 total)...\n",
      "\tProgress: 4167/4167 (100.0%)\n",
      "\tCompleted in 13.84 seconds\n",
      "\n",
      "Processing Thai sentences (4167 total)...\n",
      "\tProgress: 4167/4167 (100.0%)\n",
      "\tCompleted in 17.21 seconds\n",
      "\n",
      "THAI Summary:\n",
      "\tEnglish: 4167 sentences\n",
      "\tThai: 4167 sentences\n",
      "\tTotal time: 31.05 seconds\n",
      "\tSaved to ./embeddings/embeddings_thai.pkl\n",
      "\n",
      "======================================================================\n",
      "ALL LANGUAGES PROCESSED!\n",
      "Total time: 312.92 seconds (5.22 minutes)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#| echo: true\n",
    "#| eval: false\n",
    "\n",
    "embeddings_by_language = process_all_languages(\n",
    "    data, \n",
    "    tokenizer, \n",
    "    model, \n",
    "    batch_size=16,\n",
    "    device=None,\n",
    "    save_dir='./embeddings'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f82fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_embeddings(langs, embeddings_folder=\"embeddings\"):\n",
    "    embeddings_by_language = {}\n",
    "\n",
    "    for idx, (lang, _, _) in enumerate(langs, 1):\n",
    "        print(f\"\\n[{idx}/{len(langs)}] Loading embeddings for '{lang}'\")\n",
    "\n",
    "        embeddings_by_language[lang] = {}\n",
    "\n",
    "        # Path to saved embeddings for this language\n",
    "        filepath = os.path.join(embeddings_folder, f\"embeddings_{lang}.pkl\")\n",
    "\n",
    "        # Load embeddings from disk\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            embeddings_by_language[lang] = pickle.load(f)\n",
    "            keys = \", \".join(embeddings_by_language[lang].keys())\n",
    "            print(f\"Loaded successfully (keys: {keys})\")\n",
    "\n",
    "    print(\"\\nAll embeddings loaded.\")\n",
    "    return embeddings_by_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bb05757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/10] Loading embeddings for 'albanian'\n",
      "Loaded successfully (keys: english, non_english)\n",
      "\n",
      "[2/10] Loading embeddings for 'arabic'\n",
      "Loaded successfully (keys: english, non_english)\n",
      "\n",
      "[3/10] Loading embeddings for 'french'\n",
      "Loaded successfully (keys: english, non_english)\n",
      "\n",
      "[4/10] Loading embeddings for 'german'\n",
      "Loaded successfully (keys: english, non_english)\n",
      "\n",
      "[5/10] Loading embeddings for 'greek'\n",
      "Loaded successfully (keys: english, non_english)\n",
      "\n",
      "[6/10] Loading embeddings for 'hindi'\n",
      "Loaded successfully (keys: english, non_english)\n",
      "\n",
      "[7/10] Loading embeddings for 'japanese'\n",
      "Loaded successfully (keys: english, non_english)\n",
      "\n",
      "[8/10] Loading embeddings for 'macedonian'\n",
      "Loaded successfully (keys: english, non_english)\n",
      "\n",
      "[9/10] Loading embeddings for 'persian'\n",
      "Loaded successfully (keys: english, non_english)\n",
      "\n",
      "[10/10] Loading embeddings for 'thai'\n",
      "Loaded successfully (keys: english, non_english)\n",
      "\n",
      "All embeddings loaded.\n"
     ]
    }
   ],
   "source": [
    "embeddings_by_language = load_embeddings(langs, embeddings_folder=\"./embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d46fd35",
   "metadata": {},
   "source": [
    "Checking that all embeddings have been extracted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bee0b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All embeddings validated successfully.\n"
     ]
    }
   ],
   "source": [
    "#| echo: true\n",
    "#| output: true\n",
    "\n",
    "def validate_word_embeddings(data, embeddings_by_language):\n",
    "    for lang in data.keys():\n",
    "        english_embeddings = embeddings_by_language[lang]['english']\n",
    "        non_english_embeddings = embeddings_by_language[lang]['non_english']\n",
    "\n",
    "        # Sanity check: sentence-level alignment\n",
    "        if len(english_embeddings) != len(non_english_embeddings):\n",
    "            print(\"FAILED\")\n",
    "            return False\n",
    "\n",
    "        # Check all sentence embeddings\n",
    "        for en_sent, non_en_sent in zip(english_embeddings, non_english_embeddings):\n",
    "            if len(en_sent['word_embeddings']) != en_sent['num_words']:\n",
    "                return False\n",
    "            if len(non_en_sent['word_embeddings']) != non_en_sent['num_words']:\n",
    "                return False\n",
    "\n",
    "    return print(\"All embeddings validated successfully.\")\n",
    "\n",
    "validate_word_embeddings(data, embeddings_by_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f82aad",
   "metadata": {},
   "source": [
    "## First alignement method: argmax over cosine similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee30c0a",
   "metadata": {},
   "source": [
    "Right now we have one embedding per word. That is, for each example, we have computed a contextualised embedding. Since our goal is to build a lexicon, we need to aggregate these embeddings across all examples.\n",
    "\n",
    "The simplest way to do this is to average the embeddings for each word in the vocabulary. This way, we obtain a single embedding per word type. But this approach has some drawbacks:\n",
    "\n",
    "- It ignores the fact that words can have multiple meanings (polysemy).\n",
    "- It treats all occurrences of a word equally, regardless of context.\n",
    "- It can be sensitive to outliers, as rare contexts can skew the average.\n",
    "- It does not capture the full distribution of meanings a word can have.\n",
    "- It assumes that the average embedding is a good representation of the word's meaning!\n",
    "\n",
    "The \"correct\" way to take into account multiple meanings would be to use a sense inventory and cluster the embeddings based on these senses. However, this requires external resources that we currently do not have. Furthermore, even if we had such inventories, how would we know for each occurrence which sense is being used? The contextualised embeddings already capture some of this information, but we would need a way to link them to the senses. For example we could compute the average embedding for each sense from the inventory and then assign each occurrence to the closest sense embedding...\n",
    "\n",
    "A \"better\" way to do this given our resources is to use clustering. For each word, we can cluster its contextualised embeddings and then use the centroids of these clusters as the word's representations. This way, we can capture multiple meanings of a word and reduce the impact of outliers. But this approach also has its challenges. For example, choosing the right number of clusters can be tricky, how can we decide how many senses a word has?\n",
    "\n",
    "For this exercise, we will stick to the simple averaging method. Either way, it is a good step to start with the simplest method and see if it works well enough for our purposes.\n",
    "\n",
    "For each language, we will build a lexicon by word type, averaging the embeddings for each word across all its occurrences.\n",
    "\n",
    "- One function to build the lexicon for a given language.\n",
    "- A loop to build lexicons for all languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762dffb",
   "metadata": {},
   "source": [
    "We also want to drop all invalid words (punctuation, special tokens, numbers, etc.). We can use a simple regex to do this. We can take advantage of python's `str.isalpha()` method, which checks if a character is alphabetic in any script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d2ef5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_word(w):\n",
    "    # Drop pure numbers\n",
    "    if w.isdigit():\n",
    "        return False\n",
    "    # Keep only tokens with at least one alphabetic character (any script)\n",
    "    if not any(ch.isalpha() for ch in w):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64a14c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_lexicon_for_language(embeddings):\n",
    "    word_to_embeddings = defaultdict(list)\n",
    "\n",
    "    for sent in embeddings:\n",
    "        words = sent['sentence'].split()\n",
    "        word_embs = sent['word_embeddings']\n",
    "\n",
    "        for w, w_emb in zip(words, word_embs):\n",
    "            if not is_valid_word(w):\n",
    "                continue\n",
    "            w = w.lower()\n",
    "            word_to_embeddings[w].append(w_emb['embedding'])\n",
    "\n",
    "    return {\n",
    "        word: np.mean(np.stack(embs), axis=0)\n",
    "        for word, embs in word_to_embeddings.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82cd3f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "def build_lexicons_for_all_languages(embeddings_by_language):\n",
    "    lexicons_by_language = {}\n",
    "\n",
    "    for lang in embeddings_by_language.keys():\n",
    "        print(f\"\\nLanguage: {lang}\")\n",
    "\n",
    "        english_lexicon = build_lexicon_for_language(\n",
    "            embeddings_by_language[lang]['english']\n",
    "        )\n",
    "        non_english_lexicon = build_lexicon_for_language(\n",
    "            embeddings_by_language[lang]['non_english']\n",
    "        )\n",
    "\n",
    "        lexicons_by_language[lang] = {\n",
    "            'english': english_lexicon,\n",
    "            'non_english': non_english_lexicon\n",
    "        }\n",
    "\n",
    "        # Print unique word types\n",
    "        print(f\"English word types ({len(english_lexicon)}):\")\n",
    "\n",
    "        print(f\"Non-English word types ({len(non_english_lexicon)}):\")\n",
    "\n",
    "    return lexicons_by_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67477862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Language: albanian\n",
      "English word types (13906):\n",
      "Non-English word types (17797):\n",
      "\n",
      "Language: arabic\n",
      "English word types (13483):\n",
      "Non-English word types (18304):\n",
      "\n",
      "Language: french\n",
      "English word types (14164):\n",
      "Non-English word types (16312):\n",
      "\n",
      "Language: german\n",
      "English word types (13717):\n",
      "Non-English word types (17397):\n",
      "\n",
      "Language: greek\n",
      "English word types (14163):\n",
      "Non-English word types (18510):\n",
      "\n",
      "Language: hindi\n",
      "English word types (13702):\n",
      "Non-English word types (9297):\n",
      "\n",
      "Language: japanese\n",
      "English word types (13482):\n",
      "Non-English word types (9682):\n",
      "\n",
      "Language: macedonian\n",
      "English word types (12985):\n",
      "Non-English word types (17234):\n",
      "\n",
      "Language: persian\n",
      "English word types (10663):\n",
      "Non-English word types (8414):\n",
      "\n",
      "Language: thai\n",
      "English word types (14605):\n",
      "Non-English word types (5754):\n"
     ]
    }
   ],
   "source": [
    "#| output: true\n",
    "\n",
    "emb_lexicons_by_language = build_lexicons_for_all_languages(embeddings_by_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd03baac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: true\n",
    "#| output: true\n",
    "\n",
    "def validate_lexicons(embeddings_by_language, lexicons_by_language):\n",
    "    for lang in embeddings_by_language.keys():\n",
    "        for side in ['english', 'non_english']:\n",
    "            lexicon = lexicons_by_language[lang][side]\n",
    "\n",
    "            # Check dimensional consistency\n",
    "            dims = {v.shape for v in lexicon.values()}\n",
    "            if len(dims) != 1:\n",
    "                return False\n",
    "\n",
    "            # Check lexicon size vs total tokens\n",
    "            total_tokens = sum(\n",
    "                len(s['word_embeddings'])\n",
    "                for s in embeddings_by_language[lang][side]\n",
    "            )\n",
    "            if len(lexicon) > total_tokens:\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "validate_lexicons(embeddings_by_language, emb_lexicons_by_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81fff57",
   "metadata": {},
   "source": [
    "Right now we have embeddings for each \"word type\" in each language, but we have included whole sentences that also contain punctuation marks and special tokens. We need to filter these out to keep only valid words. We can use a simple regex to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d52c8",
   "metadata": {},
   "source": [
    "Now we can build a cosine similarity matrix for each language between the English and non-English lexicons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37c9c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def build_cosine_similarity_matrix(lexicon_non_english, lexicon_english):\n",
    "    # Extract word lists\n",
    "    non_en_words = list(lexicon_non_english.keys())\n",
    "    en_words = list(lexicon_english.keys())\n",
    "\n",
    "    # Stack embeddings into matrices\n",
    "    non_en_vectors = np.stack([lexicon_non_english[w] for w in non_en_words])\n",
    "    en_vectors = np.stack([lexicon_english[w] for w in en_words])\n",
    "\n",
    "    # Cosine similarity: rows = non-English, columns = English\n",
    "    sim_matrix = cosine_similarity(non_en_vectors, en_vectors)\n",
    "\n",
    "    return sim_matrix, non_en_words, en_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "similarity_matrices = {}\n",
    "\n",
    "for lang, lexicons in emb_lexicons_by_language.items():\n",
    "    sim_matrix, non_en_words, en_words = build_cosine_similarity_matrix(\n",
    "        lexicons['non_english'],\n",
    "        lexicons['english']\n",
    "    )\n",
    "\n",
    "    similarity_matrices[lang] = {\n",
    "        'matrix': sim_matrix,\n",
    "        'non_english_words': non_en_words,\n",
    "        'english_words': en_words\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c794fd41",
   "metadata": {},
   "source": [
    "We can now build lexicons for all languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a9e9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "def build_lexicon(similarity_matrices, k=1):\n",
    "    lexicons = {}\n",
    "\n",
    "    for lang, sim_data in similarity_matrices.items():\n",
    "        sim_matrix = sim_data['matrix']\n",
    "        non_en_words = sim_data['non_english_words']\n",
    "        en_words = sim_data['english_words']\n",
    "\n",
    "        lexicon = {}\n",
    "        for i, non_en_word in enumerate(non_en_words):\n",
    "            sims = sim_matrix[i]\n",
    "\n",
    "            if k == 1:\n",
    "                best_idx = int(np.argmax(sims))\n",
    "                lexicon[non_en_word] = [en_words[best_idx]]\n",
    "            else:\n",
    "                topk_idx = np.argsort(sims)[-k:][::-1]\n",
    "                lexicon[non_en_word] = [en_words[j] for j in topk_idx]\n",
    "\n",
    "        lexicons[lang] = lexicon\n",
    "\n",
    "    return lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7a76c",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "There might be a slight encoding problem, the print statements show empty strings for some languages but this is only a problem for latex. The actual strings are there.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62d1f108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Language: albanian\n",
      "Sample lexicon entries:\n",
      "\tfaqja -> ['photo']\n",
      "\tdy -> ['two']\n",
      "\tçfarë -> ['how']\n",
      "\tkemi -> ['to']\n",
      "\tmësuar? -> ['up.']\n",
      "\tkanë -> ['have']\n",
      "\tkaluar -> ['over']\n",
      "\tpesëdhjetë -> ['four']\n",
      "\tvjet -> ['years']\n",
      "\tqë -> ['which']\n",
      "\n",
      "Language: arabic\n",
      "Sample lexicon entries:\n",
      "\tكيف -> ['what']\n",
      "\tيمكنني -> ['others']\n",
      "\tان -> ['that']\n",
      "\tاضبط -> ['addicted']\n",
      "\tعادات -> ['habits']\n",
      "\tمشاهدتي -> ['recordings']\n",
      "\tالتلفزيون -> ['radio']\n",
      "\tربما -> ['day']\n",
      "\tطرحتم -> ['concluded:']\n",
      "\tعلى -> ['on']\n",
      "\n",
      "Language: french\n",
      "Sample lexicon entries:\n",
      "\téloge -> ['mystery']\n",
      "\tdes -> ['of']\n",
      "\ttémoins -> ['whom']\n",
      "\tl’œuvre -> ['which']\n",
      "\tde -> ['of']\n",
      "\tprédication -> ['efforts']\n",
      "\tjéhovah -> ['jehovah']\n",
      "\ts’étend -> ['which']\n",
      "\trapidement. -> ['run']\n",
      "\tcette -> ['this']\n",
      "\n",
      "Language: german\n",
      "Sample lexicon entries:\n",
      "\tein -> ['a']\n",
      "\twort -> ['word']\n",
      "\tzugunsten -> ['against']\n",
      "\tder -> ['the']\n",
      "\tzeugen -> ['witnesses']\n",
      "\tjehovas -> ['jehovah’s']\n",
      "\tdas -> ['is']\n",
      "\tpredigtwerk -> ['worldwide']\n",
      "\tdehnt -> ['changed']\n",
      "\tsich -> ['while']\n",
      "\n",
      "Language: greek\n",
      "Sample lexicon entries:\n",
      "\tένας -> ['one']\n",
      "\tκαλός -> ['than']\n",
      "\tλόγος -> ['which']\n",
      "\tγια -> ['for']\n",
      "\tτους -> ['them']\n",
      "\tμάρτυρας -> ['four']\n",
      "\tη -> ['the']\n",
      "\tδρασις -> ['paraguay']\n",
      "\tκηρύγματος -> ['testimony']\n",
      "\tτων -> ['of']\n",
      "\n",
      "Language: hindi\n",
      "Sample lexicon entries:\n",
      "\tबड़ा -> ['large']\n",
      "\tव्यवसाय -> ['military']\n",
      "\tऔर -> ['and']\n",
      "\tअपराध -> ['against']\n",
      "\tव्यवसाय! -> ['fascinating,']\n",
      "\tहम -> ['us']\n",
      "\tसब -> ['all']\n",
      "\tपर -> ['on']\n",
      "\tइसका -> ['its']\n",
      "\tप्रभाव -> ['effect']\n",
      "\n",
      "Language: japanese\n",
      "Sample lexicon entries:\n",
      "\t「エホバ -> ['other']\n",
      "\tの -> ['and']\n",
      "\t証人 -> ['witnesses,']\n",
      "\tへ -> ['and']\n",
      "\t賛辞」 -> ['scribes,']\n",
      "\tエホバ -> ['and']\n",
      "\t伝道 -> ['ps.']\n",
      "\t活動 -> ['and']\n",
      "\tは -> ['and']\n",
      "\t急速 -> ['and']\n",
      "\n",
      "Language: macedonian\n",
      "Sample lexicon entries:\n",
      "\tмладите -> ['young']\n",
      "\tпрашуваат... -> ['walk']\n",
      "\tкако -> ['as']\n",
      "\tможам -> ['could']\n",
      "\tда -> ['to']\n",
      "\tсе -> ['known']\n",
      "\tсправам -> ['cope']\n",
      "\tсо -> ['with']\n",
      "\tнеправдата? -> ['injustice']\n",
      "\tги -> ['them']\n",
      "\n",
      "Language: persian\n",
      "Sample lexicon entries:\n",
      "\tinternational -> ['society']\n",
      "\tbible -> ['bible']\n",
      "\tstudents -> ['society']\n",
      "\tassociation -> ['society']\n",
      "\tاین -> ['this']\n",
      "\tنشریه -> ['publication']\n",
      "\tبرای -> ['for']\n",
      "\tفروش -> ['sale.']\n",
      "\tنیست -> ['is']\n",
      "\tو -> ['and']\n",
      "\n",
      "Language: thai\n",
      "Sample lexicon entries:\n",
      "\tตอน -> ['near']\n",
      "\tที่ -> ['where']\n",
      "\tจาก -> ['from']\n",
      "\tส. -> ['year']\n",
      "\tศ. -> ['c.e.,']\n",
      "\tเป็น -> ['once']\n",
      "\tต้น -> ['state']\n",
      "\tมา -> ['mixed']\n",
      "\tสิ่ง -> ['enough']\n",
      "\tถือ -> ['mixed']\n"
     ]
    }
   ],
   "source": [
    "#| output: true\n",
    "\n",
    "argmax_lexicons = build_lexicon(similarity_matrices)\n",
    "\n",
    "for lang, lexicon in argmax_lexicons.items():\n",
    "    print(f\"\\nLanguage: {lang}\")\n",
    "    print(f\"Sample lexicon entries:\")\n",
    "    sample_items = list(lexicon.items())[:10]\n",
    "    for non_en_word, en_word in sample_items:\n",
    "        print(f\"\\t{non_en_word} -> {en_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f79da",
   "metadata": {},
   "source": [
    "The first results are not great... we can also create \"top-k\" lexicons to see if the correct translation is among the top candidates. For example k=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7e7e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "top5_lexicons = build_lexicon(similarity_matrices, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8934a2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Language: albanian\n",
      "Sample lexicon entries:\n",
      "\tfaqja -> ['photo', 'perhaps.', 'mirror', 'ela', 'falter']\n",
      "\tdy -> ['two', 'three', 'four', 'third', 'second']\n",
      "\tçfarë -> ['how', 'did', 'what', 'whether', 'then']\n",
      "\tkemi -> ['to', 'which', 'them', 'and', 'they']\n",
      "\tmësuar? -> ['up.', 'them', 'to', 'it.', 'them.']\n",
      "\tkanë -> ['have', 'least', 'been', 'they', 'had']\n",
      "\tkaluar -> ['over', 'before', 'period', 'long', 'last']\n",
      "\tpesëdhjetë -> ['four', 'five', 'half', 'years', 'ten']\n",
      "\tvjet -> ['years', 'year', 'years.', 'years,', 'ago.']\n",
      "\tqë -> ['which', 'that', 'and', 'as', 'they']\n",
      "\n",
      "Language: arabic\n",
      "Sample lexicon entries:\n",
      "\tكيف -> ['what', 'how', 'why', 'whether', 'what,']\n",
      "\tيمكنني -> ['others', 'lead', 'learn', 'avoid', 'could']\n",
      "\tان -> ['that', 'it', 'leave', 'purpose', 'last']\n",
      "\tاضبط -> ['addicted', 'recklessly', 'again', 'addiction', 'raising']\n",
      "\tعادات -> ['habits', 'behavior', 'known', 'hours,', 'them']\n",
      "\tمشاهدتي -> ['recordings', 'reports', 'between', 'friends', 'their']\n",
      "\tالتلفزيون -> ['radio', 'middle', 'over', 'and', 'reports']\n",
      "\tربما -> ['day', 'perhaps', 'often', 'may', 'times']\n",
      "\tطرحتم -> ['concluded:', 'set', 'evolution', 'yourself', 'thought']\n",
      "\tعلى -> ['on', 'off', 'at', 'over', 'place']\n",
      "\n",
      "Language: french\n",
      "Sample lexicon entries:\n",
      "\téloge -> ['mystery', '“the', '“life', 'isa.', 'matt.']\n",
      "\tdes -> ['of', 'or', 'and', 'which', 'some']\n",
      "\ttémoins -> ['whom', 'which', 'witnesses.', 'witnesses,', 'and']\n",
      "\tl’œuvre -> ['which', 'on', 'the', 'an', 'and']\n",
      "\tde -> ['of', 'which', 'and', 'or', 'less']\n",
      "\tprédication -> ['efforts', 'work', 'purpose', 'work,', 'service']\n",
      "\tjéhovah -> ['jehovah', 'jehovah’s', 'witnesses', 'john', 'god,']\n",
      "\ts’étend -> ['which', 'of', 'and', 'second', 'wide']\n",
      "\trapidement. -> ['run', 'it.', 'long', 'along', 'hard']\n",
      "\tcette -> ['this', 'these', 'which', 'such', 'the']\n",
      "\n",
      "Language: german\n",
      "Sample lexicon entries:\n",
      "\tein -> ['a', 'an', 'and', 'while', 'or']\n",
      "\twort -> ['word', 'god,', 'god.', 'holy', 'things.']\n",
      "\tzugunsten -> ['against', 'toward', 'instead', 'bringing', 'taken']\n",
      "\tder -> ['the', 'of', 'which', 'that', 'and']\n",
      "\tzeugen -> ['witnesses', 'jehovah’s', 'persons', 'jehovah', 'members']\n",
      "\tjehovas -> ['jehovah’s', 'jehovah', 'witnesses', 'jehovah.', 'god,']\n",
      "\tdas -> ['is', 'is,', 'that', 'and', 'it']\n",
      "\tpredigtwerk -> ['worldwide', 'holy', 'matt.', 'preaching', 'gigantic']\n",
      "\tdehnt -> ['changed', 'further', 'of', 'second', 'increased']\n",
      "\tsich -> ['while', 'and', 'was', 'become', 'them']\n",
      "\n",
      "Language: greek\n",
      "Sample lexicon entries:\n",
      "\tένας -> ['one', 'another', 'a', 'an', 'other']\n",
      "\tκαλός -> ['than', 'national', 'for', 'four', 'a']\n",
      "\tλόγος -> ['which', 'the', 'of', 'on', 'law']\n",
      "\tγια -> ['for', 'which', 'hold', 'and', 'it.']\n",
      "\tτους -> ['them', 'whom', 'them.', 'their', 'them,']\n",
      "\tμάρτυρας -> ['four', 'whom', 'three', 'matt.', 'and']\n",
      "\tη -> ['the', 'which', 'is', 'it', 'and']\n",
      "\tδρασις -> ['paraguay', 'rectifier', 'stitch', 'argentina', 'condenser']\n",
      "\tκηρύγματος -> ['testimony', 'clergy', 'second', 'matt.', 'book']\n",
      "\tτων -> ['of', 'which', 'them', 'those', 'whom']\n",
      "\n",
      "Language: hindi\n",
      "Sample lexicon entries:\n",
      "\tबड़ा -> ['large', 'great', 'more', 'small', 'up']\n",
      "\tव्यवसाय -> ['military', 'which', 'great', 'toxic', 'major']\n",
      "\tऔर -> ['and', 'which', 'or', 'as', 'more']\n",
      "\tअपराध -> ['against', 'threat', 'act', 'or', 'which']\n",
      "\tव्यवसाय! -> ['fascinating,', 'great', 'big', 'awake!', 'success']\n",
      "\tहम -> ['us', 'others', 'clearly', 'must', 'there']\n",
      "\tसब -> ['all', 'its', 'them', 'most', 'and']\n",
      "\tपर -> ['on', 'of', 'for', 'against', 'which']\n",
      "\tइसका -> ['its', 'it', 'which', 'this', 'makes']\n",
      "\tप्रभाव -> ['effect', 'causes', 'of', 'effects', 'on']\n",
      "\n",
      "Language: japanese\n",
      "Sample lexicon entries:\n",
      "\t「エホバ -> ['other', 'prov.', 'further', 'bible', 'national']\n",
      "\tの -> ['and', 'of', 'which', 'or', 'neither']\n",
      "\t証人 -> ['witnesses,', 'witnesses', 'neither', 'whom', 'and']\n",
      "\tへ -> ['and', 'kingdom', 'from', 'his', 'whom']\n",
      "\t賛辞」 -> ['scribes,', 'servant', 'teaching', 'welcome', 'glad']\n",
      "\tエホバ -> ['and', 'father', 'his', 'witnesses,', 'kingdom']\n",
      "\t伝道 -> ['ps.', 'of', 'and', 'peaceful', 'national']\n",
      "\t活動 -> ['and', 'which', 'kingdom', 'ps.', 'purpose']\n",
      "\tは -> ['and', 'of', 'or', 'neither', 'which']\n",
      "\t急速 -> ['and', 'of', 'on', 'it.', 'less']\n",
      "\n",
      "Language: macedonian\n",
      "Sample lexicon entries:\n",
      "\tмладите -> ['young', 'children', 'old', 'youngsters', 'men']\n",
      "\tпрашуваат... -> ['walk', 'known', 'observe', 'isaiah', 'they']\n",
      "\tкако -> ['as', 'which', 'known', 'and', 'well']\n",
      "\tможам -> ['could', 'can', 'able', 'ever', 'known']\n",
      "\tда -> ['to', 'make', 'known', 'them', 'until']\n",
      "\tсе -> ['known', 'are', 'as', 'well', 'were']\n",
      "\tсправам -> ['cope', 'well', 'and', 'known', 'doing']\n",
      "\tсо -> ['with', 'and', 'including', 'which', 'whom']\n",
      "\tнеправдата? -> ['injustice', 'what', 'contact', 'discouraged', 'exceeded']\n",
      "\tги -> ['them', 'known', 'they', 'and', 'were']\n",
      "\n",
      "Language: persian\n",
      "Sample lexicon entries:\n",
      "\tinternational -> ['society', 'assembly', 'watch', 'tower', 'bible']\n",
      "\tbible -> ['bible', 'society', 'bible,', 'bible?', 'bible’s']\n",
      "\tstudents -> ['society', 'york,', 'tower', 'tract', 'pennsylvania']\n",
      "\tassociation -> ['society', 'pennsylvania', 'watch', 'york,', 'tower']\n",
      "\tاین -> ['this', 'these', 'the', 'such', 'other']\n",
      "\tنشریه -> ['publication', 'published', 'publications', 'article', 'distribution.']\n",
      "\tبرای -> ['for', 'form', 'family.', 'purpose', 'perhaps']\n",
      "\tفروش -> ['sale.', 'publication', 'distribution.', 'publications', 'download']\n",
      "\tنیست -> ['is', 'not', 'no', 'only', 'simply']\n",
      "\tو -> ['and', 'thick', 'which', 'or', 'of']\n",
      "\n",
      "Language: thai\n",
      "Sample lexicon entries:\n",
      "\tตอน -> ['near', 'before', 'revelation', 'special', 'or']\n",
      "\tที่ -> ['where', 'once', 'at', 'which', 'when']\n",
      "\tจาก -> ['from', 'into', 'per', 'revelation', 'through']\n",
      "\tส. -> ['year', 'state', 'new', 'kingdom', 'world']\n",
      "\tศ. -> ['c.e.,', 'c.e.', 's.', 'south', 'u.s.']\n",
      "\tเป็น -> ['once', 'finest', 'a', '“there', 'was']\n",
      "\tต้น -> ['state', 'or', 'back', 'world', 'once']\n",
      "\tมา -> ['mixed', 'state', 'once', 'great', 'new']\n",
      "\tสิ่ง -> ['enough', 'his', 'mixed', 'once', 'time']\n",
      "\tถือ -> ['mixed', 'once', 'near', 'which', 'never']\n"
     ]
    }
   ],
   "source": [
    "#| output: true\n",
    "\n",
    "for lang, lexicon in top5_lexicons.items():\n",
    "    print(f\"\\nLanguage: {lang}\")\n",
    "    print(f\"Sample lexicon entries:\")\n",
    "    sample_items = list(lexicon.items())[:10]\n",
    "    for non_en_word, en_word in sample_items:\n",
    "        print(f\"\\t{non_en_word} -> {en_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2ba06",
   "metadata": {},
   "source": [
    "Looking at the first entries (I consider only the languages I can speak, French, Greek and German), there seems to be an ever so slight improvement when considering the top-5 candidates. What seems to me the case more often than not is that functional words are translated correctly more often than content words. Furthermore, there seems to be a \"pooling\" effect, where functional words get high similarity scores with content words and thus appear in the top translation-candidate position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e0b1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "def sentence_level_hit_at_k(src_sentences, tgt_sentences, lexicon):\n",
    "    tgt_word_sets = [set(sent.split()) for sent in tgt_sentences]\n",
    "    accuracies = []\n",
    "\n",
    "    for src_sent, tgt_word_set in zip(src_sentences, tgt_word_sets):\n",
    "        hits = 0\n",
    "        total = 0\n",
    "\n",
    "        for w in src_sent.split():\n",
    "            if w not in lexicon:\n",
    "                continue\n",
    "\n",
    "            predicted_words = set(lexicon[w])\n",
    "\n",
    "            if predicted_words & tgt_word_set:\n",
    "                hits += 1\n",
    "\n",
    "            total += 1\n",
    "\n",
    "        accuracies.append(hits / total if total > 0 else 0.0)\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56a33534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_all_languages(data, translation_lexicons):\n",
    "    rows = []\n",
    "\n",
    "    for lang, lexicon in translation_lexicons.items():\n",
    "        accuracies = sentence_level_hit_at_k(\n",
    "            src_sentences=data[lang]['pairs']['non_english'],\n",
    "            tgt_sentences=data[lang]['pairs']['english'],\n",
    "            lexicon=lexicon\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"language\": lang,\n",
    "            \"hit\": float(np.mean(accuracies))\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7be9bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_k1 = evaluate_all_languages(\n",
    "    data,\n",
    "    argmax_lexicons\n",
    ")\n",
    "\n",
    "results_k5 = evaluate_all_languages(\n",
    "    data,\n",
    "    top5_lexicons\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "678d0f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| language   |       hit |\n",
       "|:-----------|----------:|\n",
       "| albanian   | 0.275912  |\n",
       "| arabic     | 0.173384  |\n",
       "| french     | 0.307206  |\n",
       "| german     | 0.362282  |\n",
       "| greek      | 0.313626  |\n",
       "| hindi      | 0.194358  |\n",
       "| japanese   | 0.245503  |\n",
       "| macedonian | 0.336152  |\n",
       "| persian    | 0.156475  |\n",
       "| thai       | 0.0587208 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: true\n",
    "#| label: results_k1\n",
    "#| tbl-cap: \"Results for argmax evaluation across all languages.\"\n",
    "\n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "Markdown(results_k1.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c08b2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| language   |      hit |\n",
       "|:-----------|---------:|\n",
       "| albanian   | 0.493644 |\n",
       "| arabic     | 0.341002 |\n",
       "| french     | 0.525088 |\n",
       "| german     | 0.585316 |\n",
       "| greek      | 0.530915 |\n",
       "| hindi      | 0.410893 |\n",
       "| japanese   | 0.512551 |\n",
       "| macedonian | 0.551903 |\n",
       "| persian    | 0.304214 |\n",
       "| thai       | 0.20467  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: true\n",
    "#| label: results_k5\n",
    "#| tbl-cap: \"Results for top 5 evaluation across all languages.\"\n",
    "\n",
    "Markdown(results_k5.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeea66e",
   "metadata": {},
   "source": [
    "Our evaluation method is \"hit at k\", meaning that we check if the correct translation is among the top k candidates.\n",
    "\n",
    "Concretely, we have build lexicons for each language, where each source word is mapped to its top k most similar target words based on cosine similarity of the mean of their contextualized embeddings. One manual way to evaluate is to simply look at those lexicons and see if the translations make sense, but of course, this is not very informative...\n",
    "\n",
    "What we want to do is to look at the pairs of sentences in our dataset, and for each source word in the sentence, check if its translation (or set of candidate translations) is present in the target sentence. This approach can at least inform us if on the sentence level, our lexicon is able to capture some of the correct translations. This of course completely ignores the alignement, but either way, since languages have different word orders, this is a reasonable first step.\n",
    "\n",
    "More importantly, this evaluation is flawed in that it does not account for the errors introduced by the alignement method itself. For example, if a source word is aligned to an incorrect target word, say a functional word, if that functional word happens to be present in the target sentence, we would count that as a hit, even though the alignment was incorrect. This means that our evaluation might overestimate the quality of our lexicon.\n",
    "\n",
    "When we look at the results across all languages, the performance is quite low, with hit@1 scores around 10-30% and hit@5 scores around 20-60%. It is not surprising that the performance increases when considering the top 5 candidates instead of just the top 1, but it can also overestimate the lexicon quality due to the reasons mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f272b7",
   "metadata": {},
   "source": [
    "## Second alignement method: Competitive linking using the Hungarian algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5f20c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def build_lexicon_competitive_linking(similarity_matrices):\n",
    "    lexicons = {}\n",
    "\n",
    "    for lang, sim_data in similarity_matrices.items():\n",
    "        print(f\"Building competitive lexicon for language: {lang}\")\n",
    "\n",
    "        sim_matrix = sim_data[\"matrix\"]\n",
    "        non_en_words = sim_data[\"non_english_words\"]\n",
    "        en_words = sim_data[\"english_words\"]\n",
    "\n",
    "        # Hungarian algorithm minimizes cost, so negate similarities\n",
    "        cost_matrix = -sim_matrix\n",
    "\n",
    "        row_idx, col_idx = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "        lexicon = {}\n",
    "        for i, j in zip(row_idx, col_idx):\n",
    "            lexicon[non_en_words[i]] = en_words[j]\n",
    "\n",
    "        lexicons[lang] = lexicon\n",
    "        print(f\"Linked {len(lexicon)} word pairs\\n\")\n",
    "\n",
    "    return lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55383b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building competitive lexicon for language: albanian\n",
      "Linked 13906 word pairs\n",
      "\n",
      "Building competitive lexicon for language: arabic\n",
      "Linked 13483 word pairs\n",
      "\n",
      "Building competitive lexicon for language: french\n",
      "Linked 14164 word pairs\n",
      "\n",
      "Building competitive lexicon for language: german\n",
      "Linked 13717 word pairs\n",
      "\n",
      "Building competitive lexicon for language: greek\n",
      "Linked 14163 word pairs\n",
      "\n",
      "Building competitive lexicon for language: hindi\n",
      "Linked 9297 word pairs\n",
      "\n",
      "Building competitive lexicon for language: japanese\n",
      "Linked 9682 word pairs\n",
      "\n",
      "Building competitive lexicon for language: macedonian\n",
      "Linked 12985 word pairs\n",
      "\n",
      "Building competitive lexicon for language: persian\n",
      "Linked 8414 word pairs\n",
      "\n",
      "Building competitive lexicon for language: thai\n",
      "Linked 5754 word pairs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "competitive_lexicons = build_lexicon_competitive_linking(similarity_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c05edaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Language: albanian\n",
      "Sample lexicon entries:\n",
      "\tfaqja -> falter\n",
      "\tdy -> two\n",
      "\tçfarë -> did\n",
      "\tkemi -> we\n",
      "\tmësuar? -> learned?\n",
      "\tkanë -> have\n",
      "\tkaluar -> leaving\n",
      "\tpesëdhjetë -> fifty\n",
      "\tvjet -> years\n",
      "\tqë -> that\n",
      "\n",
      "Language: arabic\n",
      "Sample lexicon entries:\n",
      "\tكيف -> learning.\n",
      "\tيمكنني -> fidgeting,\n",
      "\tان -> complaining\n",
      "\tعادات -> manners\n",
      "\tمشاهدتي -> media\n",
      "\tالتلفزيون -> watcher.\n",
      "\tربما -> occasionally\n",
      "\tعلى -> mainland.\n",
      "\tأنفسكم -> “every\n",
      "\tهذا -> ritual?\n",
      "\n",
      "Language: french\n",
      "Sample lexicon entries:\n",
      "\téloge -> deluge\n",
      "\tdes -> perhaps\n",
      "\ttémoins -> congregations\n",
      "\tl’œuvre -> upbuilding\n",
      "\tde -> of\n",
      "\tprédication -> work,\n",
      "\tjéhovah -> jehovah’s\n",
      "\ts’étend -> watchtower\n",
      "\trapidement. -> masse.\n",
      "\tcette -> this\n",
      "\n",
      "Language: german\n",
      "Sample lexicon entries:\n",
      "\tein -> inexperienced\n",
      "\twort -> word\n",
      "\tder -> the\n",
      "\tzeugen -> witnesses,\n",
      "\tjehovas -> witnesses\n",
      "\tdas -> provides\n",
      "\tpredigtwerk -> work:\n",
      "\tsich -> kept\n",
      "\tsehr -> very\n",
      "\tschnell -> fast\n",
      "\n",
      "Language: greek\n",
      "Sample lexicon entries:\n",
      "\tένας -> one\n",
      "\tκαλός -> simpler\n",
      "\tλόγος -> account\n",
      "\tγια -> for\n",
      "\tτους -> them\n",
      "\tμάρτυρας -> did:\n",
      "\tη -> everyday\n",
      "\tτων -> prophecies\n",
      "\tμαρτύρων -> relics\n",
      "\tτου -> of\n",
      "\n",
      "Language: hindi\n",
      "Sample lexicon entries:\n",
      "\tबड़ा -> huge\n",
      "\tव्यवसाय -> business.\n",
      "\tऔर -> and\n",
      "\tअपराध -> illegal\n",
      "\tव्यवसाय! -> fascinating,\n",
      "\tहम -> we\n",
      "\tसब -> all,\n",
      "\tपर -> on\n",
      "\tइसका -> makes\n",
      "\tप्रभाव -> effect\n",
      "\n",
      "Language: japanese\n",
      "Sample lexicon entries:\n",
      "\t「エホバ -> james\n",
      "\tの -> the\n",
      "\t証人 -> witness\n",
      "\tへ -> back\n",
      "\t賛辞」 -> scribes,\n",
      "\tエホバ -> witnesses\n",
      "\t伝道 -> preaching\n",
      "\t活動 -> purpose\n",
      "\tは -> was\n",
      "\t急速 -> produce\n",
      "\n",
      "Language: macedonian\n",
      "Sample lexicon entries:\n",
      "\tмладите -> youths\n",
      "\tпрашуваат... -> drama\n",
      "\tкако -> as\n",
      "\tможам -> can’t\n",
      "\tда -> to\n",
      "\tсе -> being\n",
      "\tсправам -> calm\n",
      "\tсо -> with\n",
      "\tнеправдата? -> misread\n",
      "\tги -> them\n",
      "\n",
      "Language: persian\n",
      "Sample lexicon entries:\n",
      "\tinternational -> tower\n",
      "\tbible -> bible?\n",
      "\tstudents -> york,\n",
      "\tassociation -> society\n",
      "\tاین -> this\n",
      "\tنشریه -> publication\n",
      "\tبرای -> for\n",
      "\tفروش -> sale.\n",
      "\tنیست -> wise,\n",
      "\tو -> going.\n",
      "\n",
      "Language: thai\n",
      "Sample lexicon entries:\n",
      "\tตอน -> threats\n",
      "\tที่ -> environment\n",
      "\tจาก -> internal\n",
      "\tส. -> antioch,\n",
      "\tศ. -> c.e.,\n",
      "\tเป็น -> finest\n",
      "\tต้น -> respective\n",
      "\tมา -> preparing\n",
      "\tสิ่ง -> caused\n",
      "\tถือ -> always\n"
     ]
    }
   ],
   "source": [
    "#| output: true\n",
    "\n",
    "for lang, lexicon in competitive_lexicons.items():\n",
    "    print(f\"\\nLanguage: {lang}\")\n",
    "    print(f\"Sample lexicon entries:\")\n",
    "    sample_items = list(lexicon.items())[:10]\n",
    "    for non_en_word, en_word in sample_items:\n",
    "        print(f\"\\t{non_en_word} -> {en_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89b852a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_competitive_linking = evaluate_all_languages(\n",
    "    data,\n",
    "    competitive_lexicons\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0804f",
   "metadata": {},
   "source": [
    "We can then evaluate the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37788419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| language   |       hit |\n",
       "|:-----------|----------:|\n",
       "| albanian   | 0.0892302 |\n",
       "| arabic     | 0.142443  |\n",
       "| french     | 0.11539   |\n",
       "| german     | 0.0888936 |\n",
       "| greek      | 0.111314  |\n",
       "| hindi      | 0.11673   |\n",
       "| japanese   | 0.111532  |\n",
       "| macedonian | 0.0868781 |\n",
       "| persian    | 0.0777913 |\n",
       "| thai       | 0.114054  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: true\n",
    "#| label: results_cl\n",
    "#| tbl-cap: \"Results for competitive linking evaluation across all languages.\"\n",
    "\n",
    "Markdown(results_competitive_linking.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8930ce6",
   "metadata": {},
   "source": [
    "The results are considerably worse, but they have curiously improved for Thai. With a one-to-one alignement, we avoid the \"pooling\" effect where functional words get aligned to multiple content words, which was leading to many false positives in the previous evaluation.\n",
    "\n",
    "We can also have a look at the distribution of similarity scores for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b37f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def collect_similarity_distributions(similarity_matrices, competitive_lexicons):\n",
    "    results = {}\n",
    "\n",
    "    for lang, sim_data in similarity_matrices.items():\n",
    "        sim_matrix = sim_data[\"matrix\"]\n",
    "        non_en_words = sim_data[\"non_english_words\"]\n",
    "        en_words = sim_data[\"english_words\"]\n",
    "\n",
    "        # Argmax similarities\n",
    "        argmax_sim = np.max(sim_matrix, axis=1)\n",
    "\n",
    "        # Hungarian similarities (only matched words)\n",
    "        lexicon = competitive_lexicons[lang]\n",
    "        src_to_idx = {w: i for i, w in enumerate(non_en_words)}\n",
    "        tgt_to_idx = {w: i for i, w in enumerate(en_words)}\n",
    "\n",
    "        hungarian_sim = []\n",
    "        for src_w, tgt_w in lexicon.items():\n",
    "            i = src_to_idx[src_w]\n",
    "            j = tgt_to_idx[tgt_w]\n",
    "            hungarian_sim.append(sim_matrix[i, j])\n",
    "\n",
    "        results[lang] = {\n",
    "            \"argmax\": argmax_sim,\n",
    "            \"hungarian\": np.array(hungarian_sim)\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b83850e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_argmax_vs_hungarian(sim_distributions):\n",
    "    languages = list(sim_distributions.keys())\n",
    "    n_langs = len(languages)\n",
    "\n",
    "    argmax_data = [sim_distributions[lang][\"argmax\"] for lang in languages]\n",
    "    hungarian_data = [sim_distributions[lang][\"hungarian\"] for lang in languages]\n",
    "\n",
    "    x = np.arange(n_langs)\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    bp1 = plt.boxplot(\n",
    "        argmax_data,\n",
    "        positions=x - width / 2,\n",
    "        widths=width,\n",
    "        patch_artist=True,\n",
    "        showfliers=False\n",
    "    )\n",
    "\n",
    "    bp2 = plt.boxplot(\n",
    "        hungarian_data,\n",
    "        positions=x + width / 2,\n",
    "        widths=width,\n",
    "        patch_artist=True,\n",
    "        showfliers=False\n",
    "    )\n",
    "\n",
    "    # Color the boxes\n",
    "    for box in bp1[\"boxes\"]:\n",
    "        box.set_facecolor(\"lightblue\")\n",
    "    for box in bp2[\"boxes\"]:\n",
    "        box.set_facecolor(\"lightgreen\")\n",
    "\n",
    "    plt.xticks(x, languages, rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Cosine similarity\")\n",
    "\n",
    "    plt.legend(\n",
    "        [bp1[\"boxes\"][0], bp2[\"boxes\"][0]],\n",
    "        [\"Argmax\", \"Hungarian\"],\n",
    "        loc=\"upper right\"\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c764ee57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| output: true\n",
    "#| label: sim_distributions_plot\n",
    "#| fig-cap: \"Box plots comparing similarity distributions for Argmax and Competitive Linking methods across languages.\"\n",
    "\n",
    "sim_distributions = collect_similarity_distributions(\n",
    "    similarity_matrices,\n",
    "    competitive_lexicons\n",
    ")\n",
    "\n",
    "plot_argmax_vs_hungarian(sim_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59088f6",
   "metadata": {},
   "source": [
    "Across all languages, argmax consistently yields higher median and upper-range similarities, reflecting its greedy nature: it selects the strongest possible match for each source word, often reusing frequent English words. Competitive linking shows lower medians, confirming that enforcing one-to-one alignments limits the maximum achievable similarity scores ,as \"hub\" words cannot be reused. There seems not to be a significant difference between languages, the median values for both methods (around 0.5-0.6) are relatively consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808540fa",
   "metadata": {},
   "source": [
    "## Third alignement method: CCA-based mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbfa21a",
   "metadata": {},
   "source": [
    "The starting point is to load the bilingual lexicons from `MUSE` as well as the previously computed embeddings from `mBERT`. We will describe the pipeline for one language.\n",
    "\n",
    "- Load the bilingual lexicon from `MUSE`.\n",
    "- Filter to pairs covered by the precomputed mBERT word-type lexicons, and extract the corresponding embeddings to form aligned training matrices.\n",
    "- Train a CCA model to learn linear projections for the two languages.\n",
    "- Project all source and target word embeddings into the shared CCA space.\n",
    "- Compute cosine similarity matrix in the CCA space.\n",
    "- Repeat for all languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2e9c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd05617",
   "metadata": {},
   "source": [
    "Building the aligned training matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a640883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "def build_cca_training_data(src_lex, tgt_lex, gold_pairs):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for src_w, tgt_w in gold_pairs:\n",
    "        if src_w in src_lex and tgt_w in tgt_lex:\n",
    "            X.append(src_lex[src_w])\n",
    "            Y.append(tgt_lex[tgt_w])\n",
    "\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb79e00",
   "metadata": {},
   "source": [
    "Compute the CCA projections. Concerning the components, the documentation says: \"number of components to keep. Should be in [1, min(n_samples, n_features, n_targets)]\". So we do something similar by setting a fixed upper bound of 50 components and then clamping the actual number of components to the maximum value allowed by the data, namely the minimum of the number of gold pairs minus one and the embedding dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "def train_cca(X, Y, max_components=50):\n",
    "    n_pairs, dim = X.shape\n",
    "    n_components = min(max_components, n_pairs - 1, dim)\n",
    "    cca = CCA(n_components=n_components, max_iter=2000)\n",
    "    cca.fit(X, Y)\n",
    "    return cca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "def build_cca_similarity_matrix(src_lex, tgt_lex, cca):\n",
    "    src_words = list(src_lex.keys())\n",
    "    tgt_words = list(tgt_lex.keys())\n",
    "\n",
    "    X = np.stack([src_lex[w] for w in src_words])\n",
    "    Y = np.stack([tgt_lex[w] for w in tgt_words])\n",
    "\n",
    "    # Project both vocabularies jointly into the CCA space\n",
    "    Xp, Yp = cca.transform(X, Y)\n",
    "\n",
    "    sim = cosine_similarity(Xp, Yp)\n",
    "    return sim, src_words, tgt_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3f726",
   "metadata": {},
   "source": [
    "Apply the pipeline for one language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "def cca_align_language(lang, lexicons_by_language, data, max_components=50, min_pairs=100):\n",
    "    src_lex = lexicons_by_language[lang][\"non_english\"]\n",
    "    tgt_lex = lexicons_by_language[lang][\"english\"]\n",
    "    gold_pairs = data[lang][\"lexicon\"]\n",
    "\n",
    "    X, Y = build_cca_training_data(src_lex, tgt_lex, gold_pairs)\n",
    "\n",
    "    if X.shape[0] < min_pairs:\n",
    "        print(f\"Skipping {lang}: only {X.shape[0]} gold pairs\")\n",
    "        return None\n",
    "\n",
    "    cca = train_cca(X, Y, max_components)\n",
    "    return build_cca_similarity_matrix(src_lex, tgt_lex, cca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53605913",
   "metadata": {},
   "source": [
    "... and for all languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880e5fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "def cca_align_all_languages(lexicons_by_language, data, max_components=50, min_pairs=100):\n",
    "    cca_similarity_matrices = {}\n",
    "\n",
    "    for lang in lexicons_by_language:\n",
    "        print(f\"Training CCA for {lang}\")\n",
    "        results = cca_align_language(\n",
    "            lang,\n",
    "            lexicons_by_language,\n",
    "            data,\n",
    "            max_components=max_components,\n",
    "            min_pairs=min_pairs\n",
    "        )\n",
    "\n",
    "        if results is None:\n",
    "            continue\n",
    "\n",
    "        sim, src_words, tgt_words = results\n",
    "        cca_similarity_matrices[lang] = {\n",
    "            \"matrix\": sim,\n",
    "            \"non_english_words\": src_words,\n",
    "            \"english_words\": tgt_words\n",
    "        }\n",
    "\n",
    "    return cca_similarity_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d7f5512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CCA for albanian\n",
      "Training CCA for arabic\n",
      "Skipping arabic: only 4 gold pairs\n",
      "Training CCA for french\n",
      "Training CCA for german\n",
      "Training CCA for greek\n",
      "Skipping greek: only 1 gold pairs\n",
      "Training CCA for hindi\n",
      "Skipping hindi: only 2 gold pairs\n",
      "Training CCA for japanese\n",
      "Skipping japanese: only 0 gold pairs\n",
      "Training CCA for macedonian\n",
      "Skipping macedonian: only 10 gold pairs\n",
      "Training CCA for persian\n",
      "Skipping persian: only 4 gold pairs\n",
      "Training CCA for thai\n",
      "Skipping thai: only 6 gold pairs\n"
     ]
    }
   ],
   "source": [
    "cca_similarity_matrices = cca_align_all_languages(\n",
    "    emb_lexicons_by_language,\n",
    "    data,\n",
    "    max_components=50,\n",
    "    min_pairs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b327f6",
   "metadata": {},
   "source": [
    "Now we can build the lexicons in the CCA space and evaluate them using the same  hit@k method as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9b71ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "argmax_caa_lexicons = build_lexicon(cca_similarity_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f9ac21fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| language   |        hit |\n",
       "|:-----------|-----------:|\n",
       "| albanian   | 0.00322573 |\n",
       "| french     | 0.00987898 |\n",
       "| german     | 0.00315783 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: true\n",
    "#| label: results_caa_k1\n",
    "#| tbl-cap: \"Results for CCA argmax evaluation across all languages.\"\n",
    "\n",
    "results_cca = evaluate_all_languages(\n",
    "    data,\n",
    "    argmax_caa_lexicons\n",
    ")\n",
    "\n",
    "Markdown(results_cca.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad66558",
   "metadata": {},
   "source": [
    "The results are pretty terrible, but this has certainly to do with the very poor evaluation we have been using so far. We can perhaps use the `MUSE` lexicons as a gold standard for evaluation instead of the sentence-level co-occurrence method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8976eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "def evaluate_lexicon_hit_at_k(induced_lexicon, muse_pairs, k=1):\n",
    "    # Build gold lookup since MUSE may have multiple targets per source\n",
    "    gold_lookup = {}\n",
    "    for src, tgt in muse_pairs:\n",
    "        gold_lookup.setdefault(src, set()).add(tgt)\n",
    "\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    for src_word, gold_targets in gold_lookup.items():\n",
    "        if src_word not in induced_lexicon:\n",
    "            continue\n",
    "\n",
    "        predicted = induced_lexicon[src_word][:k]\n",
    "\n",
    "        if any(tgt in gold_targets for tgt in predicted):\n",
    "            hits += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    return hits / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c25ae1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_caa_with_muse(argmax_cca_lexicons, data, k=1):\n",
    "    rows = []\n",
    "\n",
    "    for lang, lexicon in argmax_cca_lexicons.items():\n",
    "        muse_pairs = data[lang][\"lexicon\"]\n",
    "\n",
    "        # Wrap argmax predictions into lists to reuse the Hit@k logic\n",
    "        wrapped_lexicon = {\n",
    "            src: tgt if isinstance(tgt, list) else [tgt]\n",
    "            for src, tgt in lexicon.items()\n",
    "        }\n",
    "\n",
    "        acc = evaluate_lexicon_hit_at_k(\n",
    "            wrapped_lexicon,\n",
    "            muse_pairs,\n",
    "            k=k\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"language\": lang,\n",
    "            f\"hit@{k}\": acc,\n",
    "            \"n_gold\": len(muse_pairs)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "707a9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_caa_lexicons5 = build_lexicon(cca_similarity_matrices, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25d3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| language   |    hit@1 |   n_gold |\n",
       "|:-----------|---------:|---------:|\n",
       "| albanian   | 0.553191 |    11491 |\n",
       "| french     | 0.698189 |    10872 |\n",
       "| german     | 0.651899 |    14677 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: true\n",
    "#| label: results_caa_detailed\n",
    "#| tbl-cap: \"Detailed results for CCA argmax evaluation across all languages.\"\n",
    "\n",
    "detailed_results_cca = evaluate_caa_with_muse(\n",
    "    argmax_caa_lexicons,\n",
    "    data\n",
    ")\n",
    "\n",
    "Markdown(detailed_results_cca.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2333c8",
   "metadata": {},
   "source": [
    "Yes, this is actually much better! Lets see with k=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9f77e2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| language   |    hit@5 |   n_gold |\n",
       "|:-----------|---------:|---------:|\n",
       "| albanian   | 0.553191 |    11491 |\n",
       "| french     | 0.700201 |    10872 |\n",
       "| german     | 0.651899 |    14677 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: true\n",
    "#| label: results_caa_detailed2\n",
    "#| tbl-cap: \"Detailed results for CCA top 5 evaluation across all languages.\"\n",
    "\n",
    "detailed_results_cca5 = evaluate_caa_with_muse(\n",
    "    argmax_caa_lexicons5,\n",
    "    data,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "Markdown(detailed_results_cca5.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d481103",
   "metadata": {},
   "source": [
    "There is only an improvement for French."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
