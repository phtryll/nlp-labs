{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e49c23",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Word alignment with multilingual BERT: from subwords to lexicons\"\n",
    "subtitle: \"Multilingual NLP -- Lab 2\"\n",
    "author: \"Philippos Triantafyllou\"\n",
    "date-modified: last-modified\n",
    "date-format: long\n",
    "lang: en\n",
    "format: html\n",
    "theme: cosmo\n",
    "toc: true\n",
    "number-sections: true\n",
    "number-depth: 2\n",
    "code-line-numbers: true\n",
    "echo: true\n",
    "output: true\n",
    "cap-location: top\n",
    "embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b83b84b",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "## Instructions\n",
    "\n",
    "In this lab, we will explore how to extract a bilingual lexicon (a list of word translation pairs linking a source language to a target language) directly from the internal representations of a \"large\" multilingual language model. Specifically, we will rely on `mBERT`, a variant of `BERT` pre-trained on Wikipedia in over 100 languages (Devlin et al. 2019).\n",
    "\n",
    "As we discussed at (too) great length in class, one of the surprising findings about `mBERT` is its ability to perform cross-lingual transfer without any explicit alignment objective or parallel data (Pires, Schlinger et Garrette 2019; Wu et Dredze 2019). Although trained solely with a masked language modelling objective on monolingual corpora, `mBERT` appears to learn a shared semantic space across languages. Words that are translations of each other tend to occupy neighbouring regions in the representation space, even when the languages do not share scripts or subwords.\n",
    "\n",
    "Because of this emergent alignment, it is possible to recover word-level translation pairs by comparing the contextualised embeddings produced by `mBERT` for parallel sentences. In this lab, we will experiment with three different alignment strategies:\n",
    "\n",
    "- Direct argmax alignment, where each source word is linked to the target word with the most similar embedding.\n",
    "- Competitive linking (via the Hungarian algorithm), which enforces one-to-one alignments.\n",
    "- Canonical Correlation Analysis (CCA), which learns linear projections to better align the representation spaces of two languages (Cao, Kitaev et Klein 2020).\n",
    "\n",
    "To carry out this lab, we will rely on a range of multilingual resources. First, we require parallel corpora (collections of sentence pairs that are translations of each other) so that we can compare word representations across languages. Well-known examples include the No Language Left Behind (`NLLB`) dataset (Team et al. 2022), which provides large-scale, high-quality translations across more than 200 languages. In addition, we will use bilingual lexicons, that is, precompiled lists of word translation pairs such as those available in `MUSE` (Lample et al. 2018) or `PanLex` (Kamholz, Pool et Colowick 2014). These lexicons serve both as supervision signals (for instance when applying Canonical Correlation Analysis) and as gold standards to evaluate the accuracy of the alignments we obtain.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507e202",
   "metadata": {},
   "source": [
    "## Selecting languages\n",
    "\n",
    "We select 5 languages that are well represented in the training of `mBERT` and 5 others that are either less represented or absent (the only language that is completely absent from `mBERT` is Kurdish). All the languages also have a bilingual lexicon from `MUSE` that we will use for the CCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85593d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (language, code, link_to_MUSE)\n",
    "langs = [\n",
    "    (\"albanian\",    \"sq\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-sq.0-5000.txt\"),\n",
    "    (\"arabic\",      \"ar\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-ar.0-5000.txt\"),\n",
    "    (\"french\",      \"fr\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-fr.0-5000.txt\"),\n",
    "    (\"german\",      \"de\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-de.0-5000.txt\"),\n",
    "    (\"greek\",       \"el\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-el.0-5000.txt\"),\n",
    "    (\"hindi\",       \"hi\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.0-5000.txt\"),\n",
    "    (\"japanese\",    \"ja\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-ja.0-5000.txt\"),\n",
    "    (\"macedonian\",  \"mk\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-mk.0-5000.txt\"),\n",
    "    (\"persian\",     \"fa\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-fa.0-5000.txt\"),\n",
    "    (\"thai\",        \"th\",   \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-th.0-5000.txt\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe8c7260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datasets import load_dataset\n",
    "\n",
    "def install_datasets(langs):\n",
    "    corpora = {}\n",
    "\n",
    "    for lang, code, links in langs:\n",
    "        print(f\"Processing {lang}...\")\n",
    "        corpora[lang] = {}\n",
    "        \n",
    "        print(f\"\\tLoading pairs...\")\n",
    "        data = load_dataset(\n",
    "            \"sentence-transformers/parallel-sentences-jw300\",\n",
    "            f\"en-{code}\",\n",
    "            split=\"train\"\n",
    "        )\n",
    "        print(f\"\\tLoaded JW300 for {lang}\")\n",
    "        corpora[lang]['pairs'] = data.take(5000) # type: ignore\n",
    "\n",
    "        print(f\"\\tLoading lexicons...\")\n",
    "        text = requests.get(links).content.decode(\"utf-8\", errors=\"ignore\")\n",
    "        lexicon = [tuple(line.split(None, 1)) for line in text.splitlines() if line.strip()]\n",
    "        corpora[lang][\"lexicon\"] = lexicon\n",
    "\n",
    "    return corpora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
